---
title: "ARC_BOLD_v5_checks"
author: "MCarew"
date: "2025-04-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This code will prepare data for checking against BOLD version 5. Here it is applied to the entire ARC dataset (including data in the )

# read in all ARC datafiles and includubg synonyms file (find_synonyms.R and bins file (from appendix 3)
```{r}
# load libraries
library(dplyr)
library(stringr)

# Set base directory
wd <- "~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/asv_library"
wd1 <- "~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow"

# Read in synonyms output from Chris's find_synonyms.R
tfill <- readxl::read_excel(paste0(wd, "/asv_library_corrected.xlsx"), sheet = 1)
maxp <- readxl::read_excel(paste0(wd, "/asv_library_corrected.xlsx"), sheet = 2)

# Read miseq13 and drop 'per' and 'site_per' columns (so data frame is the same format as others)
miseq13 <- read.csv(paste0(wd1, "/miseq13/metabarcoding_workflow/ten_site_data_summaries/data_for_analysis/ten_sites_long_format_25_02_2025.csv")) %>%
  dplyr::select(-"per", -"site_per")

# Read other Miseq datasets
miseq15 <- read.csv(paste0(wd1, "/miseq15/metabarcoding_workflow/final_modeling_data/miseq15_final_data_long_format_17_02_2025.csv"))
miseq16 <- read.csv(paste0(wd1, "/miseq16/metabarcoding_workflow/final_modeling_data/miseq16_final_data_long_format_17_02_2025.csv"))
miseq17 <- read.csv(paste0(wd1, "/miseq17/metabarcoding_workflow/final_modeling_data/miseq17_final_data_long_format_17_02_2025.csv"))
miseq18 <- read.csv(paste0(wd1, "/miseq18/metabarcoding_workflow/final_modeling_data/miseq18_final_data_long_format_26_02_2025.csv"))
miseq19 <- read.csv(paste0(wd1, "/miseq19/metabarcoding_workflow/final_modeling_data/miseq19_final_data_long_format_18_02_2025.csv"))
miseq20 <- read.csv(paste0(wd1, "/miseq20/metabarcoding_workflow/final_modeling_data/miseq20_final_data_long_format_18_02_2025.csv"))

# conbine all arc data into a single file
all_arc <- rbind(miseq13, miseq15, miseq16, miseq17, miseq18, miseq19, miseq20)

# files are large so remove after combining
rm(miseq13, miseq15, miseq16, miseq17, miseq18, miseq19, miseq20)

# read in synonyms
synonyms <- read.csv("~/uomShare/wergStaff/MelCarew/git-data/Spring_2018_DNA_metabarcoding_data/synonym_updates/synonyms_2025.csv")

# read in bins
bins <- read.csv("~/uomShare/wergStaff/MelCarew/git-data/Spring_2018_DNA_metabarcoding_data/synonym_updates/bins.csv")

```

Pull out the data needed for checking the species classification. This code selects asv's that is not in the tfill or max_p_identity tables (maxp). CHRIS: please note we did not include the data from miseq13 (the 10 site study) in the current library (my fault). Tt will be captured here and will be data with original species matches (max_p_identity) of >97 that do not appear in the asv library.
```{r}
# pulling species data for checking (and remove duplicate asvs)
all_arc <- all_arc %>%
  select(asv_code, asv_seq, phylum, order, family, genus, species, max_p_identity) %>%
  distinct()

# Combine the asv_code values from tfill and maxp
codes_to_remove <- union(tfill$asv_code, maxp$asv_code)

# Filter arc_all to remove matching asv_code values (isolate asv without a species match)
all_arc_unmatched <- all_arc %>%
  filter(!asv_code %in% codes_to_remove)

# add information from synonym table (adds a new column with any matches to the synonym table)
all_arc_unmatched <- all_arc_unmatched %>%
  left_join(synonyms %>% select(species, species_new), by = "species")

# save file for check against the bold barcodeID engine manually (this can be coded in future)

write.csv(all_arc_unmatched, paste0(wd, "/all_arc_unmatched_for_sequence_search.csv"), row.names = FALSE)
```

OK. Now I have searched all the asv sequences against against BOLD version 5 using the 'rapid species search' against the 'public+private' library manually. The 'rapid species search' can take 1000 records at a time. However, sometimes you can't download the final 'combined hits' possibly the.csv file is too large (so less sequences need to be searched)  This code brings the bold 'combined hits' .csv back into R.
```{r}
#set directory for results files
wd3 <- "~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/asv_library/unmatched_boldv5_search_results"

# Read in combined hits .csv files with BOLD barcodeID engine results
bold_results1 <- read.csv(paste0(wd3, "/all_arc_unmatched_1_1000_BOLD_results.csv"))
bold_results2 <- read.csv(paste0(wd3, "/all_arc_unmatched_1001_2000_BOLD_results.csv"))
bold_results3 <- read.csv(paste0(wd3, "/all_arc_unmatched_2001_3000_BOLD_results.csv"))
bold_results4 <- read.csv(paste0(wd3, "/all_arc_unmatched_3001_4000_BOLD_results.csv"))
bold_results5 <- read.csv(paste0(wd3, "/all_arc_unmatched_4001_4821_BOLD_results.csv"))
bold_results6 <- read.csv(paste0(wd3, "/all_arc_unmatched_4821_end_BOLD_results.csv"))

# combine all results into a single file
bold_results_all <- rbind(bold_results1, bold_results2, bold_results3, bold_results4, bold_results5, bold_results6)

# files are large so remove after combining
rm(bold_results1, bold_results2, bold_results3, bold_results4, bold_results5, bold_results6)

# rename all results to match other dataframes
bold_results_all <- bold_results_all %>%
  rename(asv_code = Query.ID) %>%
  rename(bold_species = Species)

#  extract 'bin_url' and place in a new column
bold_results_all <- bold_results_all %>%
  mutate(bin_uri = str_extract(PID..BIN., "(?<=:)(.*?)(?=])"))

# join with 'bins' to add current species names (from our database and previuos checks)
bold_results_all <- bold_results_all %>%
  left_join(bins, by = "bin_uri")
```

Here, we perpare the data from the bold search so it can be added to the asvs NOT in our current asv library (e.g. tfill or max-p-identity)
```{r}
# summarize the bold data keeping the highest value for ID (note this doesn't always give the lowest taxonomy, just the best matching record). Remove PID..BIN. column so duplicate otu_id can be removed.
bold_results_all <- bold_results_all %>%
  select(-"PID..BIN.")

# summarise so there is one  otu_id per row
bold_results_sum <- bold_results_all %>%
  group_by(`asv_code`) %>%
  slice_max(order_by = `ID.`, n = 1) %>%
  ungroup() %>%
  distinct(asv_code, .keep_all = TRUE) # Add this line to remove fully duplicate rows
```

Here, we add the bold data into the unmatched asv file (all_arc_unmatched)
```{r}
# check bold IDs against synonym IDs and and previous identifications (orig_species)
# Perform a left join to join bold ids to other ids
arc_unmatched_ck <- all_arc_unmatched %>%
  left_join(bold_results_sum %>% select(asv_code, species, ID., bin_uri), by = "asv_code") 

# update column names to avoid confusion
arc_unmatched_ck  <- arc_unmatched_ck  %>%
  rename(species_orig = species.x, # original names
         species_bold = species.y, # names from bold v5 check
         species_syn = species_new, # names from synonym df
         bold_match = ID.,# % match from bold v5 check
         bold_bin_uri = bin_uri) # top bin match from bold v5 check

# exacting file to manually check species_orig, species_syn and species_bold plus bold_match and max-p-identities
write.csv(arc_unmatched_ck, paste0(wd, "/arc_unmatched_ck_for_checking.csv", row.names = FALSE))
```

CHRIS: Here is the data from my manual checks (below). I suspect you may have a better way to do this in R and could check my checks. I have added two new columns: 'checked_species_name' and 'comments'. The checked species name is what the name should be change to (note not all names change from the orig_species) before adding the data to the asv library. It uses the name we have previously agreed on (source from the 'bins' file). There are two main types data in terms of what we need to add. There are the records from (likely from) the miseq13 run which have a 'max_p_identiy' and 'bold_match' above 97. These records can be added to the 'tfill' sheet. There are also new species revealed by searches against bold version 5 that which previously had max-p-identities below 97 but now with a 'bold_match' above 97. These records should be added to the 'max-p-update' sheet as the max-p-identities will require changing too. The are also a few records that may need row base update as they were fungus or bacteria on further checking. I suggest we keep a comprehensive bin database of keeping all new bins (including non-macros) for matching our current specie names in the future from the BOLD version 5 database (below:arc_unmatched_checked)

```{r}
# read in checked file
arc_unmatched_checked <- read.csv(paste0(wd, "/arc_unmatched_checked.csv"))
```

Next is to extract the records for 'tfill', 'max-p-update' and 'bins' and put them in the same format as the 'asv library' spreadsheets.

```{r}
# Still need to do
```

# max-p-update update
Now I am checking all records in the max-p-update file against the BOLD version 5. This should update any new species or names (also any remaining errors)

```{r}
# extract data for manual searches against bold version 5
write.csv(maxp, "~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/asv_library/asv_library_maxp_for_sequence_search.csv")
```

As above: I have searched all the asv sequences against against BOLD version 5 using the 'rapid species search' against the 'public+private' library manually. The 'rapid species search' can take 1000 records at a time. However, sometimes you can't download the final 'combined hits' possibly the.csv file is too large (so less sequences need to be searched)  This code brings the bold 'combined hits' .csv back into R.
```{r}
wd4 <- "~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/asv_library/maxp_boldv5_search_results"

# read in .csv files with bold barcodeID engine results.
bold_results_maxp1 <- read.csv(paste0(wd4, "/maxp_1_1000BOLD_results.csv"))
bold_results_maxp2 <- read.csv(paste0(wd4, "/maxp_1000_endBOLD_results.csv"))

# combine all results into a single file
bold_results_maxp_all <- rbind(bold_results_maxp1, bold_results_maxp2)

rm(bold_results_maxp1, bold_results_maxp2)

# rename all results to match other dataframes
bold_results_maxp_all <- bold_results_maxp_all %>%
  rename(asv_code = Query.ID) %>%
  rename(bold_species = Species)

# extract 'bin_url' and place in a new column
bold_results_maxp_all <- bold_results_maxp_all %>%
  mutate(bin_uri = str_extract(PID..BIN., "(?<=:)(.*?)(?=])"))

# join with 'bins' to add current species names
bold_results_maxp_all <- bold_results_maxp_all %>%
  left_join(bins, by = "bin_uri")
```

Here, we perpare the data from the bold search so it can be added to the maxp dataframe (max-p-update)
```{r}
# summarize the bold data keeping the highest value for ID. Remove PID..BIN. column so duplicate otu_id can be removed.
bold_results_maxp_all <- bold_results_maxp_all %>%
  select(-"PID..BIN.")

# summarise so there is one  otu_id per row
bold_results_maxp_sum <- bold_results_maxp_all %>%
  group_by(`asv_code`) %>%
  slice_max(order_by = `ID.`, n = 1) %>%
  ungroup() %>%
  distinct(asv_code, .keep_all = TRUE) # Add this line to remove fully duplicate rows
```

Here, we add the bold data and synonyms into the maxp file (arc_maxp)
```{r}
# add information from synonym table to maxp
maxp_syn <- maxp %>%
  left_join(synonyms %>% select(species, species_new), by = "species")

#CHRIS: I noticed the 'maxp_syn' table was bigger than the 'maxp' table 'they should be the same size. the extra rows are coming from species with conflicting names. 29 here. I have gone back and regenerated the 'synonyms' dataframe using your most up to date code (Appendix 3 then find_synonyms.R) and checked it. It appear in some cases multiple 'species_new' names are given for a species. Do you mind checking this? Thanks
# Count how many times each species appears in synonyms
# syn_dups <- synonyms %>%
#   count(species) %>%
#   filter(n > 1)
# print(syn_dups)
# will remove these further down as I only noticed after checking

# check bold IDs against synonym IDs and and previous identifications (orig_species)
arc_maxp <- maxp_syn %>%
  left_join(bold_results_maxp_sum %>% select(asv_code, species, ID., bin_uri), by = "asv_code") 

# update column names to avoid confusion
arc_maxp  <- arc_maxp  %>%
  rename(species_orig = species.x, # original names
         species_bold = species.y, # names from bold v5 check
         species_syn = species_new, # names from synonym df
         bold_match = ID.,# % match from bold v5 check
         bold_bin_uri = bin_uri) # top bin match from bold v5 check
```

Export records out for manual checking only checking records with >97 match on bold version 5 if the name in the library (species_orig) does not match the name in on bold (specie_bold). Other records can be left as they are for now but could be checked when new DNA barcodes become available
```{r}
# only checking records where bold had a species match of >97 ()

arc_maxp_ck <- arc_maxp %>%
filter(
    (species_orig != species_bold |
     (is.na(species_bold) | species_bold == "") & species_orig != "") &
    !is.na(bold_bin_uri) & bold_bin_uri != "" &
    bold_match >= 97
  )

write.csv(arc_maxp_ck, paste0(wd, "arc_maxp_for_checking.csv"), row.names = FALSE)

```

CHRIS: We have ots of bins to add to our database, a small wumber of corrections and some species that have naming updates needed (below: arc_maxp_checked)
```{r}
# read in checked file
arc_maxp_checked <- read.csv(paste0(wd, "/arc_maxp_checked.csv"))

#check for and remove duplicated records
any(duplicated(arc_maxp_checked$asv_code))

dup_codes <- arc_maxp_checked$asv_code[duplicated(arc_maxp_checked$asv_code)]
print(dup_codes) # 4 records duplicated

arc_maxp_checked <- arc_maxp_checked %>%
  filter(
    !(asv_code %in% c(
      "1a1dbd9a5956e0cc5beb9103b438a540",
      "71a95257a7058fe71f811027f19c5891",
      "bfd627e8a10ee7e91dbf8c661471bc63",
      "fdd197e7003d49ab1412ee331ce20fd3"
    ) & species_syn == "Naididae")
  )

```
Next is to extract the 'max-p-update' record update and new 'bins' to up date the 'asv library' spreadsheets. CHRIS: thinking about it some more is it worth placing the 'bin' in the asv library? Let me know what you think?

```{r}
# Still need to do
```

# tfill check against the BOLD v5 library
Now I am checking all records in the tfill file against the BOLD version 5. This should update any new species or names (also any remaining errors)
```{r}
write.csv(tfill, "~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/asv_library/asv_library_tfill_for_sequence_search.csv")
```

As above: I have searched all the asv sequences against against BOLD version 5 using the 'rapid species search' against the 'public+private' library manually. The 'rapid species search' can take 1000 records at a time. However, sometimes you can't download the final 'combined hits' possibly the.csv file is too large (so less sequences need to be searched)  This code brings the bold 'combined hits' .csv back into R.
```{r}
# set working directory path
wd5 <- "~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/asv_library/tfill_boldv5_search_results/"

# read in .csv files with BOLD BarcodeID engine results
bold_results_tfill1 <- read.csv(paste0(wd5, "tfill_1_600BOLD_results.csv"))
bold_results_tfill2 <- read.csv(paste0(wd5, "tfill_600_1201BOLD_results.csv"))
bold_results_tfill3 <- read.csv(paste0(wd5, "tfill_1201-1800BOLD_results.csv"))
bold_results_tfill4 <- read.csv(paste0(wd5, "tfill_1801_2200BOLD_results.csv"))
bold_results_tfill5 <- read.csv(paste0(wd5, "tfill_2201_3000BOLD_results.csv"))
bold_results_tfill6 <- read.csv(paste0(wd5, "tfill_2701_endBOLD_results.csv"))
bold_results_tfill7 <- read.csv(paste0(wd5, "tfill_3001_3600BOLD_results.csv"))
bold_results_tfill8 <- read.csv(paste0(wd5, "tfill_3601_4000BOLD_results.csv"))
bold_results_tfill9 <- read.csv(paste0(wd5, "tfill_4001_4800BOLD_results.csv"))
bold_results_tfill10 <- read.csv(paste0(wd5, "tfill_4801_5400BOLD_results.csv"))
bold_results_tfill11 <- read.csv(paste0(wd5, "tfill_5401_6000BOLD_results.csv"))
bold_results_tfill12 <- read.csv(paste0(wd5, "tfill_6001_6800BOLD_results.csv"))
bold_results_tfill13 <- read.csv(paste0(wd5, "tfill_6801_7600BOLD_results.csv"))
bold_results_tfill14 <- read.csv(paste0(wd5, "tfill_7601_8200BOLD_results.csv"))
bold_results_tfill15 <- read.csv(paste0(wd5, "tfill_8801_9400BOLD_results.csv"))
bold_results_tfill16 <- read.csv(paste0(wd5, "tfill_9400_10200BOLD_results.csv"))
bold_results_tfill17 <- read.csv(paste0(wd5, "tfill_10201_11000BOLD_results.csv"))
bold_results_tfill18 <- read.csv(paste0(wd5, "tfill_11001_11600BOLD_results.csv"))
bold_results_tfill19 <- read.csv(paste0(wd5, "tfill_11600_12200BOLD_results.csv"))
bold_results_tfill20 <- read.csv(paste0(wd5, "tfill_12201_12700BOLD_results.csv"))
bold_results_tfill21 <- read.csv(paste0(wd5, "tfill_12701_13100BOLD_results.csv"))
bold_results_tfill22 <- read.csv(paste0(wd5, "tfill_13100_endBOLD_results.csv"))

# combine all results into a single file
bold_results_tfill_all <- rbind(bold_results_tfill1, bold_results_tfill2, bold_results_tfill3, bold_results_tfill4, bold_results_tfill5, bold_results_tfill6, bold_results_tfill7, bold_results_tfill8, bold_results_tfill9, bold_results_tfill10, bold_results_tfill11, bold_results_tfill12, bold_results_tfill13, bold_results_tfill14, bold_results_tfill15,bold_results_tfill16,bold_results_tfill17, bold_results_tfill18, bold_results_tfill19, bold_results_tfill20, bold_results_tfill21, bold_results_tfill22)

rm(bold_results_tfill1, bold_results_tfill2, bold_results_tfill3, bold_results_tfill4, bold_results_tfill5, bold_results_tfill6, bold_results_tfill7, bold_results_tfill8, bold_results_tfill9, bold_results_tfill10, bold_results_tfill11, bold_results_tfill12, bold_results_tfill13, bold_results_tfill14, bold_results_tfill15,bold_results_tfill16,bold_results_tfill17, bold_results_tfill18, bold_results_tfill19, bold_results_tfill20, bold_results_tfill21, bold_results_tfill22)

# rename all results to match other dataframes
bold_results_tfill_all <- bold_results_tfill_all %>%
  rename(asv_code = Query.ID) %>%
  rename(bold_species = Species)

# extract 'bin_url' and place in a new column
bold_results_tfill_all <- bold_results_tfill_all %>%
  mutate(bin_uri = str_extract(PID..BIN., "(?<=:)(.*?)(?=])"))

# join with 'bins' to add current species names from the 'bins' database
bold_results_tfill_all <- bold_results_tfill_all %>%
  left_join(bins, by = "bin_uri")
```

Here, we perpare the data from the bold search so it can be added to the tfill dataframe
```{r}
# summarize the bold data keeping the highest value for ID. Remove PID..BIN. column so duplicate otu_id's  can be removed.
bold_results_tfill_all <- bold_results_tfill_all %>%
  select(-"PID..BIN.")

# summarise so there is one  otu_id per row
bold_results_tfill_sum <- bold_results_tfill_all %>%
  group_by(`asv_code`) %>%
  slice_max(order_by = `ID.`, n = 1) %>%
  ungroup() %>%
  distinct(asv_code, .keep_all = TRUE) # Add this line to remove fully duplicate rows
```

```{r}
# add synonyms
tfill_syn <- tfill %>%
  left_join(synonyms %>% select(species, species_new), by = "species")

#CHRIS: row difference again. 362 here. Will remove these further down as I only noticed after checking.

# check bold IDs against synonym IDs and and previous identifications (orig_species)
# Perform a left join to join bold ids to other ids
arc_tfill <- tfill_syn %>%
  left_join(bold_results_tfill_sum %>% select(asv_code, species, ID., bin_uri), by = "asv_code") 

# update column names to avoid confusion
arc_tfill  <- arc_tfill  %>%
  rename(species_orig = species.x, # original names
         species_bold = species.y, # names from bold v5 check
         species_syn = species_new, # names from synonym df
         bold_match = ID.,# % match from bold v5 check
         bold_bin_uri = bin_uri) # top bin match from bold v5 check
```

Export records out for manual checking only checking records with >97 match on bold version 5 if the name in the library (species_orig) does not match the name in on bold (specie_bold)
```{r}
# pull out records that may need updating
arc_tfill_ck <- arc_tfill %>%
filter(
    (species_orig != species_bold |
     (is.na(species_bold) | species_bold == "") & species_orig != "") &
    !is.na(bold_bin_uri) & bold_bin_uri != "" &
    bold_match >= 97
  )
write.csv(arc_tfill_ck, paste0(wd, "arc_tfill_for_checking.csv"), row.names = FALSE)
```

```{r}
# Read in checked file
arc_tfill_checked <- read.csv(paste0(wd, "/arc_tfill_checked.csv"))

# Check for and report duplicated ASV codes
any(duplicated(arc_tfill_checked$asv_code))  # TRUE or FALSE
tfill_dup_codes <- arc_tfill_checked$asv_code[duplicated(arc_tfill_checked$asv_code)]
print(tfill_dup_codes) # 85 duplicated asv records

# Extract duplicated rows
tfill_dup_asvs <- arc_tfill_checked %>%
  filter(asv_code %in% tfill_dup_codes)

# Define groups of ASV codes and matching species
hydrophilidae_asvs <- c("b9991a5c09c2b936ee2f18553f10bd3e", "f737f12a2e2b2395dfe4c164307d29de")
hydrophilidae_species <- c("Hydrophilidae sp. B-ACA3039", "Hydrophilidae")

ceratopogonidae_asvs <- c("f3f45492902b48eb7ad6cbd0f42ecd9c")
ceratopogonidae_species <- c("Ceratopogonidae sp. B-ADR4164")

austroargiolestes_asvs <- c(
  "062b799d1adcdb8abd37844eec22941a", "1a4ad4c6180eb8b54b1642c56478dff1", "1dde5432529d459675250a267b87f962",
  "2d77596c28da618c6c680b9a79621e10", "3727f376e3b97666dba0053046aabe85", "42329edf624820fd6156641277500482",
  "473a12af5311f47f3aea36772651c27a", "47d63d01bc08eed6f26c5d48db10e353", "54a321b61138a1af3d07986a95e65765",
  "6eed067b9ed65c71fc22c094c9d51b74", "71a874d507af847cd0f738467259c1a3", "794f9343860329a36e3568d982e54937",
  "7bdd4bfcd59ae83b78de6ba107a8b116", "82efcc456b3a5aad7a515f3651188a17", "83efa3b551d04aaa92f170ec50a0b8c6",
  "8c540a998add88b5f37e0d922c0a53c5", "8c85d34efd45585ac560cd57efa77d6b", "9791016b22e7ac26c4177072e0c0075a",
  "9dd15d591d8f498e9344a3f43478bee8", "a382c62c5b1101f7ee50bfabe062c262", "a78b69b32783c342885451246ec6e803",
  "ab95bbfbd0e53c3b1ecd51ac2ca8d2c0", "b2b35960764554ca3d403924d86d13b5", "c0e687706cd3150c763e2f80bc20d221",
  "cfa4c4bcb6e8226938aa407fc4cc989a", "d514e7da62f32f3e58e9eade66526a53", "dac6161e66ec6b8a8bb9319a4e9fb2fe",
  "dec9957195b4425eac5f49912e119726", "e1f489d2868a1b1dd2a86ac7bedd1933", "fc48dd933b199302ea9b9b4827c3a6de"
)
austroargiolestes_species <- c("Austroargiolestes")

bayardella_asvs <- c(
  "0e6ca6da348567b55bb0f2c4480b8269", "13ca4d32d2ac9a5f59897109d2e779de", "1b94c2656d7680b696f2ec7241d46f38",
  "220817ce0eb8f8922d1ab784f054753a", "22562ba01c1407a80d104d6ca1575598", "22f5d8665b170fe4f73de578ccf8d661",
  "3210e5f5ac800b7f4bfbab60bda0ebd7", "345ffa1f5549dac7c7b4f16d4ac26575", "36aa126ab17a79d6e4c506b2976ad64a",
  "378af4b67bb8371aa93a7b3a5518d201", "38ebbfb9ce8b62f4d24cd7a1751fc134", "4215f49e43aa75242b3716146e167605",
  "4e63d7df1d5201ded5c6c2292bc30320", "57819f0c7457e05f2e9bd362281cd316", "7e4cbcb670e0c1796a71959724875e08",
  "80315130d03fc894d4bcc1134f450815", "86af43cb207e71f4310462dbfe78fe4c", "884f7d58f4989d76d35858983ab0eb17",
  "891581eae36a83b1f00661a13fe95054", "902e3ab8f4e68b269edd79667fa3f37f", "9695107a34897a126e5bf785c7a73c73",
  "9f79f7f1ade6c2d387dba5969631c438", "a1fd05888ae0113f3e36cc954573547a", "a77c28c7fdef2a41ef7222af4c214406",
  "a8dc176d194e5fe9361dd1320a912e8f", "ab105146970772d04f5e0596a091ac98", "ab50a8dff76cfeda3552eb09e13a10db",
  "b9b514dcb904e951a25da99710a7dd2c", "d01d2c3e5cb682d55b68f6bfd054df33", "d740db7102dc565393420413a9378c33",
  "d840cc07e39a90ec34876c90ad81b37e", "db80656b4bbaf67fb439b47ee80b08db", "e2f4b1ced118b9d9048d3b6d63eadc3b",
  "e9f07efa5059a3f9d3700b29d9da3668", "f382149b16383f29c58e0484fccf40cf", "f5f9eef0a2bdc0c2cfaf6ea0100303a2"
)
bayardella_species <- c("Bayardella")

leptoperla_asvs <- c(
  "3408845fd9ef21e7650dcad05a2bc40d", "3efbee16ebf463ad158886c892929348", "47e0583ede4af7ace488c318a62d4dfc",
  "53345b75d7ca3d05abb9634f2fd40299", "561da9dd81fca9e5cfa921032f7d8e0a", "5cc9638f2ab5917366ed5513b19209e4",
  "777e20cbc1fd426f3cd5c450285f4b9a", "7e7ea18fe8ad76cabc391eff25411bec", "8d36e89492d3a60e388f1d007a7acfe0",
  "ae4a051349d0a89092a47cee8d211f4e", "c33838fa95091384ab55af0d7e4d52c5", "cbd27fe246730af7a90d46bc9e119baf",
  "ce6a4c1e933398fddc2cc99fbd1b71a2", "d369e5554fdfce2a54e7579b0dd930d4", "ec13862ea7f22006b6d0ceb698cfed5a"
)
leptoperla_species <- c("Leptoperla")

# Apply filtering
arc_tfill_checked <- arc_tfill_checked %>%
  filter(
    !(
      (asv_code %in% hydrophilidae_asvs & species_syn %in% hydrophilidae_species) |
      (asv_code %in% ceratopogonidae_asvs & species_syn %in% ceratopogonidae_species) |
      (asv_code %in% austroargiolestes_asvs & species_syn %in% austroargiolestes_species) |
      (asv_code %in% bayardella_asvs & species_syn %in% bayardella_species) |
      (asv_code %in% leptoperla_asvs & species_syn %in% leptoperla_species)
    ))
```

```{r}
# check again for duplicates.
any(duplicated(arc_tfill_checked$asv_code))  # TRUE or FALSE
tfill_dup_codes <- arc_tfill_checked$asv_code[duplicated(arc_tfill_checked$asv_code)] # all good now.
```

Next is to extract the 'tfill' record to update to up date the 'asv library' spreadsheets and also any new 'bins'. 
```{r}
# Still need to do
```