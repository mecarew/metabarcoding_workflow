---
title: "Adding_additonal_taxonomic_information and replicate filtering"
author: "Melissa Carew"
date: "06/06/2024"
output: html_document
---

This notebook is designed to filling missing taxonomic assignments from the vsearch step (in 03_classification) in the metabarcoding workflow and make corrections to taxonomic assignments that are incorrectly assigned by vsearch due to errors in the DNA barcode reference libraries (SECTION 1).It also performs a final flitering to detect and remove species detections that have most likely arisen from 'mistagging' (SECTION 2)


```{r}
# Load the required libraries
library(dplyr)
library(tidyr)
library(stringr)
library(readxl)
```

## SECTION 1: Filling in missing/incorrect taxonomic classifications

A file based on analysis of Miseq datasets was constructed ('asv_library_97_jun24.xlsx'). This file captures ASVs (amplicon sequence variants) from the DADA2 step (where the metabarcoding data is reduced into a series of sequence groups with high sequence similarity i.e. ASVs) and:
-   fills truncated taxonomy assignments from the vsearch step (sheet 1)
-   groups unclassified sequences into 'OTUs' (sheet 2)
-   updates/corrects any missing identifications or errors in classification (sheet 3)
-   identifies any ambiguous taxonomic assignments that should be considered as complexes (sheet 4)

```{r}
# read in flies
file_path <- "~/Documents/asv_library_97_jun24.xlsx"

# Get the names of the sheets
sheet_names <- excel_sheets(file_path)

# Read the sheets into separate dataframes
truncated_fills_df <- readxl::read_excel(file_path, sheet = sheet_names[1]) # contains full taxonomic information for classifications with truncated taxonomy
otu_groups_df <- readxl::read_excel(file_path, sheet = sheet_names[2])# groups unclassified sequences into 'OTUs'
max_p_update_df <- readxl::read_excel(file_path, sheet = sheet_names[3])#updates/corrects any missing identifications or errors in classification
complexes_df <- readxl::read_excel(file_path, sheet = sheet_names[4])# identifies any ambiguous taxonomic assignments that should be considered as complexes

#read in raw dataframe (if starting with a clear global environment)
#final_df <- read.csv(here("results/miseq13_raw_data_summary4_Apr_24_unfiltered.csv"))
```

# remove identifications below thresholds for species species, genus, family and order
```{r}
# Update 'species' column to only include names if max threshold is above 97
final_df <-  final_df %>%
  mutate(species = if_else(max_p_identity < 97, "", species))

# Update 'genus' column to only include names if max threshold is above 95
final_df <-  final_df %>%
  mutate(genus = if_else(max_p_identity < 95, "", genus))

# Update 'family' column to only include names if max threshold is above 90
final_df <-  final_df %>%
  mutate(family = if_else(max_p_identity < 90, "", family))

# Update 'order' column to only include names if max threshold is above 80
final_df <-  final_df %>%
  mutate(order = if_else(max_p_identity < 85, "",order))
```


# Fill in truncated taxonomic assigments (using sheet 1: truncated_fills_df )
```{r}
# Find common IDs
common_ids <- intersect(truncated_fills_df$asv_code, final_df$asv_code)

print(common_ids)

df1 <- final_df[final_df$asv_code %in% common_ids, ] # asvs in common in final_df
df2 <- truncated_fills_df[truncated_fills_df$asv_code %in% common_ids, ]  # asvs in common in truncated_fills_df
df3 <-final_df[!final_df$asv_code %in% common_ids, ] # left data in final dataframe


# Update the 'species' value in df1 based on matching 'asv_code'
for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$species[i] <- df2$species[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$genus[i] <- df2$genus[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$family[i] <- df2$family[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$order[i] <- df2$order[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$class[i] <- df2$class[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$phylum[i] <- df2$phylum[df2$asv_code == target_asv_code]
}
 
for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$kingdom[i] <- df2$kingdom[df2$asv_code == target_asv_code]
}
print(df1)
```

# Add 'OTU' groupings from 'Geneious prime' denovo analysis
Any unclassified ASVs have been grouped together at a similarity threshold of <3%. These ASVs are denoted with the name 'sp. MC-ARCO'. 
```{r}
# Find common IDs
common_ids2 <- intersect(otu_groups_df$asv_code, df3$asv_code)

df4 <- df3[df3$asv_code %in% common_ids2, ] # asvs in common in remaining final_df
df5 <- otu_groups_df[otu_groups_df$asv_code %in% common_ids2, ] # asvs in common in truncated_fills_df
df6 <- df3[!df3$asv_code %in% common_ids2, ] #  data left in final dataframe

# Update the 'species' value in df1 based on matching 'asv_code'
for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$species[i] <- df5$species[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$genus[i] <- df5$genus[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$family[i] <- df5$family[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$order[i] <- df5$order[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$class[i] <- df5$class[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$phylum[i] <- df5$phylum[df5$asv_code == target_asv_code]
}
 
for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$kingdom[i] <- df5$kingdom[df5$asv_code == target_asv_code]
}

print(df4)
```
# Add in identification data for taxa missing from reference database (sheet 3)
```{r}
# Find common IDs
common_ids3 <- intersect(max_p_update_df$asv_code, final_df$asv_code)

df7 <- df6[df6$asv_code %in% common_ids3, ] # asvs in common in final_df
df8 <- max_p_update_df[max_p_update_df$asv_code %in% common_ids3, ] # asv requiring updating
df9 <- df6[!df6$asv_code %in% common_ids3, ] # data left in final dataframe

# Update the 'species' value in df1 based on matching 'asv_code'
for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$species[i] <- df8$species[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$genus[i] <- df8$genus[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$family[i] <- df8$family[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$order[i] <- df8$order[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$class[i] <- df8$class[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$phylum[i] <- df8$phylum[df8$asv_code == target_asv_code]
}
 
for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$kingdom[i] <- df8$kingdom[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$max_p_identity[i] <- df8$max_p_identity[df8$asv_code == target_asv_code]
}

print(df7)
```
# combine all filled dataframe into a single dataframe
```{r}
final_data_filled <- rbind(df1, df4, df7, df9)

# Find duplicates in the asv_code column
duplicates <- final_data_filled %>%
  filter(duplicated(asv_code) | duplicated(asv_code, fromLast = TRUE))

# Print the duplicate rows
print(duplicates)
```

# check 'final_filled_df' for any species complexes and add this information (from sheet 4)
```{r}
# Use left_join to match and then mutate to replace the species with complex
final_data_filled <- final_data_filled %>%
  left_join(complexes_df, by = c("species" = "bin_name")) %>%
  mutate(species = ifelse(!is.na(complex), complex, species)) %>%
  select(-complex)

# Print the modified dataframe
print(final_data_filled)
```
# combine all filled dataframe into a single dataframe
```{r}
final_data_filled <- rbind(df1, df4, df7, df9)

# Find duplicates in the asv_code column
duplicates <- final_data_filled %>%
  filter(duplicated(asv_code) | duplicated(asv_code, fromLast = TRUE))

# Print the duplicate rows
print(duplicates) # there should be no duplicates
```

```{r}
# save filled dataframe
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp
file_name <- paste0("~/git/metabarcoding_workflow/results/MiSeq13_final_data_summary_filled", timestamp, ".csv")

# Save the dataframe with the new file name
write.csv(final_df, file = file_name, row.names = FALSE)

# saved filled dataframe
#write.csv(final_data_filled, "~/git/metabarcoding_workflow/results/miseq13_raw_data_filled_25_jul_24.csv", row.names = FALSE)
```
______________________________

## SECTION 2: PCR filtering
This section is designed to format data and filter it for 'mistagging'. Mistagging our tag switchig can occur causing ASVs to be assigned to the wrong sample. This occurs at a low frequency and is usually random. PCR replicates included in the study design help detect and remove these errors. We use three PCR replicates and filter out any species/ASV detections that occur in only one PCR replicate (which are most likely from mistagging). We also add a second flitering step for the five most common ASV (those with the highest number of reads/sequences). These are more likely to be involved in mistagging and can appear in multiple PCR replicate we add a reads threshold for these detections.

# Change data to long formmat (list) for PCR replicate filtering 
Not this code will need to be configured for each dataset, so that the rows containing the read counts are specified (e.g. pivot_longer(cols = X10BNY29042rep1:Cont3rep1), the 'X10BNY29042rep1:Cont3rep1' will need to be changed to the first and last column name
```{r}
# Pivot longer to transform columns 13 to 40 into rows 
metab_long <- final_data_filled %>%
  pivot_longer(cols = X10BNY29042rep1:Cont3rep1, names_to = "sample", values_to = "value") %>%
  filter(value != 0) %>%  # Filter out rows where value is zero
  select(-matches("^X(\\.\\d+)?$"))  # Remove columns named X, X.1, ..., X.11

# Reorder columns with variable as the first column
metab_long <- metab_long %>%
  select(sample, everything())  # Move variable column to the first position

# Create a new column "replace" by pasting the final character if string ends in 'rep1', 'rep2', or 'rep3'
metab_long <- metab_long %>%
  mutate(replicate = ifelse(grepl("rep[123]$", sample), paste0(substr(sample, nchar(sample), nchar(sample)), ""), ""))

# Move the "replace" column to be the second column
metab_long <- metab_long %>%
  select(sample, replicate, everything())  # Move "replace" to the second column

metab_long <- metab_long %>%
  mutate(sample = str_replace(sample, "^X", ""))

# add 'factor' column
metab_long$site_per <- substr(metab_long$sample, 1, nchar(metab_long$sample))
# remove replicate data
metab_long <- metab_long %>%
   dplyr::mutate(site_per = sub("(rep1|rep2|rep3|rep4|rep5|rep6)$", "", site_per))

# Move the "replace" column to be the second column
metab_long <- metab_long %>%
  select(sample, replicate, site_per, everything())  # Move "replace" to the second column

# add 'factor' column
metab_long$site <- substr(metab_long$site_per, 1, nchar(metab_long$site_per))
# remove replicate data
metab_long <- metab_long %>%
   dplyr::mutate(site = sub("(10|20|30|40|)", "", site))

# Move the "replace" column to be the second column
metab_long <- metab_long %>%
  select(sample, replicate, site_per, site, everything())  # Move "replace" to the second column

# Rename the 'values' column to 'reads' using dplyr's rename()
metab_long <- metab_long %>% rename(`reads` = value)

# Print the transformed data frame
print(metab_long)
```
# Filtering single PCR replicate detections

# filtering by 'species' subset out data with species ids

```{r}
# Subset where 'species' has non-empty character values
metab_long_with_species <- metab_long %>% filter(!is.na(species) & species != "")

# Subset where 'species' has empty cells or NA (for ASV filtering)
metab_long_empty_species <- metab_long %>% filter(is.na(species) | species == "")
```

# remove single species occurences (one PCR replicate)
```{r}
# Count the occurrences of each combination of 'species' and 'site'
counts <- metab_long_with_species %>%
  group_by(site, species) %>%
  summarise(count = n(), .groups = 'drop')

# Filter out rows where the combination of 'site' and 'species' appears only once
sp_fil_metab_long_with_species <- metab_long_with_species %>%
  left_join(counts, by = c("site", "species")) %>%
  filter(count > 1) %>%
  select(-count)  # Remove the count column as it is no longer needed

# Print the modified dataframe
print(sp_fil_metab_long_with_species)

sp_fil_metab_long_rem <- metab_long_with_species %>% filter((species %in% counts$species[counts$count == 1] & site %in% counts$site[counts$count == 1]))
```
# filtering by ASVs
Remove ASV's with only a single occurrence in a sample (in only one PCR replicate) this targets ASV's without identifications or OTU groupings

```{r}
# Count the occurrences of each combination of 'asv_code' and 'site'
counts_asv <- metab_long_empty_species %>% group_by(asv_code, site) %>% summarise(count = n())

# Filter out rows where the count is 1 (i.e., only one occurrence)
asv_fil_metab_long <- metab_long_empty_species %>% filter(!(asv_code %in% counts_asv$asv_code[counts$count == 1] & site %in% counts$site[counts_asv$count == 1]))

```

# bind filtered dataframes
```{r}
#rejoin_dataframes
filtered_dataframe <- rbind(sp_fil_metab_long_with_species, asv_fil_metab_long)
```

# filter based on common ASV's in the dataset
```{r}
# Step 1: Group by species and summarize the total reads
species_reads <- filtered_dataframe %>%
  group_by(species) %>%
  summarise(total_reads = sum(reads, na.rm = TRUE)) %>%
  arrange(desc(total_reads))

# Print species_reads to verify the summarized data
print(species_reads)

# Step 2: Extract the top 5 species based on 'total_reads' values
top_species <- species_reads %>%
  top_n(5, wt = total_reads) %>%
  pull(species)

# Print top_species to verify the top species
print(top_species)

# Step 3: Filter out reads below 10 for the top 5 species in filtered_dataframe
filtered_dataframe <- filtered_dataframe %>%
  filter(!(species %in% top_species & reads < 10))

# Print the filtered dataframe to verify the result
print(filtered_dataframe)
```

# convert long form back to full data summary with data filled and filtered
```{r}
# Assuming final_data_filled is your dataframe
# Step 1: Pivot longer
metab_long <- final_data_filled %>%
  pivot_longer(
    cols = X10BNY29042rep1:Cont3rep1,  # Specify the range or individual column names
    names_to = "sample",
    values_to = "value"
  ) %>%
  filter(value != 0) %>%  # Filter out rows where value is zero
  select(-matches("^X(\\.\\d+)?$"))  # Remove columns named X, X.1, ..., X.11

# Step 2: Pivot wider to transform back to wide format
metab_wide <- metab_long %>%
  pivot_wider(
    names_from = sample,
    values_from = value
  )

# Remove 'X' at the start of column headings
colnames(metab_wide) <- sub("^X", "", colnames(metab_wide))

# List of columns to modify
cols_to_modify <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Convert specified columns to character type
metab_wide[cols_to_modify] <- lapply(metab_wide[cols_to_modify], as.character)

# Replace NA values with empty strings in specified columns
metab_wide[cols_to_modify][is.na(metab_wide[cols_to_modify])] <- ""

# Print the first few rows of the resulting dataframe
print(head(metab_wide))


#write_csv(metab_wide, here("results/miseq13_wide.csv"))
```
## There are a number of options 
# Summarize values by site name ()
```{r}
# Assuming flitered_dataframe is your dataframe
summarized_df <- filtered_dataframe %>%
  group_by(site, kingdom, phylum, class, order, family, genus, species, max_p_identity) %>%  # Group by specified columns including max_p_identity
  summarise(reads = sum(reads, na.rm = TRUE), .groups = 'drop') %>%  # Sum the reads for each group
  group_by(site, kingdom, phylum, class, order, family, genus, species) %>%  # Group by the main columns again
  summarise(
    reads = sum(reads, na.rm = TRUE),  # Sum the reads for each group
    max_p_identity = ifelse(all(is.na(max_p_identity)), NA, max(max_p_identity, na.rm = TRUE)),  # Handle all NA case
    .groups = 'drop'
  )

# Pivot wider to transform the site names into columns
wide_df <- summarized_df %>%
  pivot_wider(names_from = site, values_from = reads, values_fill = 0)  # Fill missing values with 0
```


```{r}
# saved filled dataframe
write.csv(final_data_filled, "~/git/metabarcoding_workflow/results/miseq13_raw_data_filled_filtered_9_jul_24.csv", row.names = FALSE)

```

