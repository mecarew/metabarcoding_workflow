---
title: "Adding_additonal_taxonomic_information and replicate filtering"
author: "Melissa Carew"
date: "06/06/2024"
output: html_document
---

This notebook is designed to filling missing taxonomic assignments from the vsearch step (in 03_classification) in the metabarcoding workflow and make corrections to taxonomic assignments that are incorrectly assigned by vsearch due to errors in the DNA barcode reference libraries (SECTION 1).It also performs a final flitering to detect and remove species detections that have most likely arisen from 'mistagging' (SECTION 2)


```{r}
# Load the required libraries
library(dplyr)
library(tidyr)
library(stringr)
library(readxl)
library(here)
```

## SECTION 1: Filling in missing/incorrect taxonomic classifications

A file based on analysis of Miseq datasets was constructed ('asv_library_97_jun24.xlsx'). This file captures ASVs (amplicon sequence variants) from the DADA2 step (where the metabarcoding data is reduced into a series of sequence groups with high sequence similarity i.e. ASVs) and this file:
-   fills truncated taxonomy assignments from the vsearch step (sheet 1)
-   groups unclassified sequences into 'OTUs' (sheet 2)
-   updates/corrects any missing identifications or errors in classification (sheet 3)
-   identifies any ambiguous taxonomic assignments that should be considered as complexes (sheet 4)

```{r}
# read in flies
file_path <- "~/Documents/asv_library_97_aug24.xlsx"

# Get the names of the sheets
sheet_names <- readxl::excel_sheets(file_path)

# Read the sheets into as dataframes
truncated_fills_df <- readxl::read_excel(file_path, sheet = sheet_names[1])  # contains full taxonomic information for classifications with truncated taxonomy
otu_groups_df <- readxl::read_excel(file_path, sheet = sheet_names[2])# groups unclassified sequences into 'OTUs'
max_p_update_df <- readxl::read_excel(file_path, sheet = sheet_names[3])#updates/corrects any missing identifications or errors in classification
complexes_df <- readxl::read_excel(file_path, sheet = sheet_names[4])# identifies any ambiguous taxonomic assignments that should be considered as complexes

# remove 'NA' from blank cell
truncated_fills_df <- truncated_fills_df %>%
  dplyr::mutate(across(everything(), ~ replace_na(., "")))
otu_groups_df <- otu_groups_df %>%
  dplyr::mutate(across(everything(), ~ replace_na(., "")))
max_p_update_df <- max_p_update_df %>%
  dplyr::mutate(across(everything(), ~ replace_na(., "")))

#read in raw dataframe (if starting with a clear global environment)
final_df <- read.csv(here::here("results/final_data_summaries/MiSeqXX_vsearch_data_summary_14_08_2024.csv")) # change the Miseq run number and time stamp
```

# remove identifications below thresholds for species species, genus, family and order
```{r}
# Update 'species' column to only include names if max threshold is above 97
final_df <-  final_df %>%
  dplyr::mutate(species = if_else(max_p_identity < 97, "", species))

# Update 'genus' column to only include names if max threshold is above 95
final_df <-  final_df %>%
  dplyr::mutate(genus = if_else(max_p_identity < 95, "", genus))

# Update 'family' column to only include names if max threshold is above 90
final_df <-  final_df %>%
  dplyr::mutate(family = if_else(max_p_identity < 90, "", family))

# Update 'order' column to only include names if max threshold is above 80
final_df <-  final_df %>%
  dplyr::mutate(order = if_else(max_p_identity < 85, "",order))
```


# Fill in truncated taxonomic assigments (using sheet 1: truncated_fills_df)
```{r}
# Find common IDs
common_ids <- intersect(truncated_fills_df$asv_code, final_df$asv_code)

print(common_ids)

df1 <- final_df[final_df$asv_code %in% common_ids, ] # asvs in common in final_df
df2 <- truncated_fills_df[truncated_fills_df$asv_code %in% common_ids, ]  # asvs in common in truncated_fills_df
df3 <-final_df[!final_df$asv_code %in% common_ids, ] # left data in final dataframe


# Update the 'species' value in df1 based on matching 'asv_code'
for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$species[i] <- df2$species[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$genus[i] <- df2$genus[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$family[i] <- df2$family[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$order[i] <- df2$order[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$class[i] <- df2$class[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$phylum[i] <- df2$phylum[df2$asv_code == target_asv_code]
}
 
for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$kingdom[i] <- df2$kingdom[df2$asv_code == target_asv_code]
}
print(df1)
```

# Add 'OTU' groupings from 'Geneious prime' denovo analysis
Any unclassified ASVs have been grouped together at a similarity threshold of <3%. These ASVs are denoted with the name 'sp. MC-ARCO'. 
```{r}
# Find common IDs
common_ids2 <- intersect(otu_groups_df$asv_code, df3$asv_code)

df4 <- df3[df3$asv_code %in% common_ids2, ] # asvs in common in remaining final_df
df5 <- otu_groups_df[otu_groups_df$asv_code %in% common_ids2, ] # asvs in common in truncated_fills_df
df6 <- df3[!df3$asv_code %in% common_ids2, ] #  data left in final dataframe

# Update the 'species' value in df1 based on matching 'asv_code'
for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$species[i] <- df5$species[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$genus[i] <- df5$genus[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$family[i] <- df5$family[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$order[i] <- df5$order[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$class[i] <- df5$class[df5$asv_code == target_asv_code]
}

for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$phylum[i] <- df5$phylum[df5$asv_code == target_asv_code]
}
 
for (i in 1:nrow(df4)) {
  target_asv_code <- df4$asv_code[i]
  df4$kingdom[i] <- df5$kingdom[df5$asv_code == target_asv_code]
}

print(df4)
```
# Add in identification data for taxa missing from reference database (sheet 3)
```{r}
# Find common IDs
common_ids3 <- intersect(max_p_update_df$asv_code, final_df$asv_code)

df7 <- df6[df6$asv_code %in% common_ids3, ] # asvs in common in final_df
df8 <- max_p_update_df[max_p_update_df$asv_code %in% common_ids3, ] # asv requiring updating
df9 <- df6[!df6$asv_code %in% common_ids3, ] # data left in final dataframe

# Update the 'species' value in df1 based on matching 'asv_code'
for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$species[i] <- df8$species[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$genus[i] <- df8$genus[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$family[i] <- df8$family[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$order[i] <- df8$order[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$class[i] <- df8$class[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$phylum[i] <- df8$phylum[df8$asv_code == target_asv_code]
}
 
for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$kingdom[i] <- df8$kingdom[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$max_p_identity[i] <- df8$max_p_identity[df8$asv_code == target_asv_code]
}

print(df7)
```
# combine all filled dataframe into a single dataframe
```{r}
final_data_filled <- rbind(df1, df4, df7, df9)
# note the "final_data_filled" and "final_df" should be the same size 

# Check if there are any duplicates in the asv_code column (there should NOT be)
duplicates <- final_data_filled %>%
  filter(duplicated(asv_code) | duplicated(asv_code, fromLast = TRUE))

# Print the duplicate rows
print(duplicates) # You should not get a table
```

# check 'final_filled_df' for any species complexes and add this information (from sheet 4)
```{r}
# Use left_join to match and then mutate to replace the species with complex
final_data_filled <- final_data_filled %>%
  left_join(complexes_df, by = c("species" = "bin_name")) %>%
  mutate(species = ifelse(!is.na(complex), complex, species)) %>%
  select(-complex)

# Print the modified dataframe
print(final_data_filled)
```

```{r}
# Save filled dataframe
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp (NOTE: Change the MiSeq number to the dataset you are working with)
file_name <- paste0(here::here("results/final_data_summaries/MiseqXX_vsearch_data_summary_filled_"), timestamp, ".csv")

# Save the dataframe with the new file name
write.csv(final_data_filled, file = file_name, row.names = FALSE)
```
______________________________

## SECTION 2: PCR filtering
This section is designed to format data and filter it for 'mistagging'. Mistagging or 'tag switching' can occur causing ASVs to be assigned to the wrong sample. This occurs at a low frequency and is usually random. PCR replicates included in the study design help detect and remove these errors. We use three PCR replicates and filter out any species/ASV detections that occur in only one PCR replicate (which are most likely from mistagging). We also add a second flitering step for the five most common ASV (those with the highest number of reads/sequences). These are more likely to be involved in mistagging and can appear in multiple PCR replicate we add a reads threshold for removing these detections.

# Change data to long formmat (list) for PCR replicate filtering 
Note: This code will need to be configured for each dataset, so that the rows containing the read counts are specified (e.g. pivot_longer(cols = X10BNY29042rep1:Cont3rep1), the 'X10BNY29042rep1:Cont3rep1' will need to be changed to the first and last column name with the reads data for each dataset
Miseq13:X10BNY29042rep1:Cont3rep1 
Miseq15:X10FEH41841rep3:S21YAR36377rep3
Miseq16:Cont1:S21WAY30960rep3
Miseq17:A22BNY29042rep1:S21YZJ1175rep6
Miseq18:A22ALD26081rep1:S22YZ8202rep3
Miseq19:A22BF3373rep1:S22YPB224rep3
Miseq20:
```{r}
# Modify column names to prefix 'X' if they start with a number
#final_data_filled <- final_data_filled %>%
  #rename_with(~ str_replace_all(., "^([0-9])", "X\\1"))

# Pivot longer to transform columns 13 to 40 into rows 
metab_long <- final_data_filled %>%
  pivot_longer(cols = X10BNY29042rep1:Cont3rep1, names_to = "sample", values_to = "value") %>%
  filter(value != 0) %>%  # Filter out rows where value is zero
  select(-matches("^X(\\.\\d+)?$"))  # Remove columns named X, X.1, ..., X.11

# Reorder columns with variable as the first column
metab_long <- metab_long %>%
  select(sample, everything())  # Move variable column to the first position

# Create a new column "replace" by pasting the final character if string ends in 'rep1', 'rep2', or 'rep3'
metab_long <- metab_long %>%
  mutate(replicate = ifelse(grepl("rep[123456]$", sample), paste0(substr(sample, nchar(sample), nchar(sample)), ""), ""))

# Move the "replace" column to be the second column
metab_long <- metab_long %>%
  select(sample, replicate, everything())  # Move "replace" to the second column

#metab_long <- metab_long %>%
#  mutate(sample = str_replace(sample, "^X", ""))

# add 'factor' column
metab_long$site_per <- substr(metab_long$sample, 1, nchar(metab_long$sample))

# remove replicate data
metab_long <- metab_long %>%
   dplyr::mutate(site_per = sub("(rep1|rep2|rep3|rep4|rep5|rep6)$", "", site_per))

# Move the "replace" column to be the second column
metab_long <- metab_long %>%
  select(sample, replicate, site_per, everything())  # Move "replace" to the second column

# add 'factor' column
metab_long$site <- substr(metab_long$site_per, 1, nchar(metab_long$site_per))

# remove replicate data (for Miseq13 only)
#metab_long <- metab_long %>% dplyr::mutate(site = sub("(X10|X20|X30|X40|)", "", site))

#Move the "replace" column to be the second column (for Miseq13 only)
#metab_long <- metab_long %>%
#select(sample, replicate, site_per, site, everything())  # Move "replace" to the second column

# Move the "replace" column to be the second column
metab_long <- metab_long %>%
  select(sample, replicate, site, everything())  # Move "replace" to the second column

# Rename the 'values' column to 'reads' using dplyr's rename()
metab_long <- metab_long %>% dplyr::rename(`reads` = value)

# Print the transformed data frame
print(metab_long)
```
# Filtering single PCR replicate detections

# filtering by 'species' subset out data with species ids
```{r}
# Subset where 'species' has non-empty character values
metab_long_with_species <- metab_long %>% filter(!is.na(species) & species != "")

# Subset where 'species' has empty cells or NA (for ASV filtering)
metab_long_empty_species <- metab_long %>% filter(is.na(species) | species == "")
```

# filter based on common species as these are more likely to be invovled in misstagging in the dataset
```{r}
# Step 1: Group by species and summarize the total reads
species_reads <- metab_long_with_species %>%
  group_by(species) %>%
  summarise(total_reads = sum(reads, na.rm = TRUE)) %>%
  arrange(desc(total_reads))

# Print species_reads to verify the summarized data
print(species_reads)

# Step 2: Extract the top 10 species detections based on 'total_reads' values
top_species <- species_reads %>%
  top_n(10, wt = total_reads) %>%
  pull(species)

# Print top_species to verify the top species
print(top_species)

# Step 3: Filter out reads below 10 for the top 10 species in filtered_dataframe
filtered_dataframe_top_sp <- metab_long_with_species %>%
  filter(!(species %in% top_species & reads < 10))

filtered_dataframe_top_sp_rem <- metab_long_with_species %>%
  filter((species %in% top_species & reads < 10))

# Verify the number of records before and after filtering
original_count <- nrow(metab_long_with_species)
filtered_count <- nrow(filtered_dataframe_top_sp)

cat("Original number of records:", original_count, "\n")
cat("Number of records after filtering:", filtered_count, "\n")
cat("Number of records removed:", original_count - filtered_count, "\n")

# Print the filtered dataframe to verify the result
print(filtered_dataframe_top_sp)
```

# remove single species occurences (one PCR replicate)
```{r}
# Remove the count column as it is no longer needed
# Step 1: Create the counts data frame
counts_sp <- filtered_dataframe_top_sp %>%
  group_by(site, species) %>%
  summarise(count = n(), .groups = 'drop')

# Step 2: Filter out rows where the combination of 'site' and 'species' appears only once
sp_fil_metab_long_with_species <- filtered_dataframe_top_sp %>%
  left_join(counts_sp, by = c("site", "species")) %>%
  filter(count > 1) %>%
  select(-count)  # Remove the count column as it is no longer needed

# Check samples removed from dataframe
sp_fil_metab_long_rem <- filtered_dataframe_top_sp %>% filter((species %in% counts_sp$species[counts_sp$count == 1] & site %in% counts_sp$site[counts_sp$count == 1]))

# Verify the number of records before and after filtering
original_count <- nrow(filtered_dataframe_top_sp)
filtered_count <- nrow(sp_fil_metab_long_with_species)

cat("Original number of records:", original_count, "\n")
cat("Number of records after filtering:", filtered_count, "\n")
cat("Number of records removed:", original_count - filtered_count, "\n")

# Print the modified dataframe
print(sp_fil_metab_long_with_species)
```

# remove single ASV occurences
Remove ASV's with only a single occurrence in a sample (in only one PCR replicate) this targets ASV's without identifications or OTU groupings
```{r}
# Step 1: Create the counts_asv data frame
counts_asv <- metab_long_empty_species %>%
  group_by(asv_code, site) %>%
  summarise(count = n(), .groups = 'drop')

# Inspect the counts_asv data frame
print(counts_asv)

# Step 2: Join and filter based on counts
asv_fil_metab_long <- metab_long_empty_species %>%
  left_join(counts_asv, by = c("asv_code", "site")) %>%
  filter(count > 1) %>%
  select(-count)  # Remove the count column as it is no longer needed

# Step 3: Verify the number of records before and after filtering
original_count <- nrow(metab_long_empty_species)
filtered_count <- nrow(asv_fil_metab_long)

cat("Original number of records:", original_count, "\n")
cat("Number of records after filtering:", filtered_count, "\n")
cat("Number of records removed:", original_count - filtered_count, "\n")
```
# bind filtered and filled dataframes
```{r}
#rejoin_dataframes
filtered_dataframe <- rbind(sp_fil_metab_long_with_species, asv_fil_metab_long)
```


# convert long form back to full data summary with data filled and filtered
Note: This code will need to be configured for each dataset, so that the rows containing the read counts are specified (e.g. pivot_longer(cols = X10BNY29042rep1:Cont3rep1), the 'X10BNY29042rep1:Cont3rep1' will need to be changed to the first and last column name
```{r}
# Step 1: Replace empty cells with 0 before pivoting
final_data_filled_clean <- final_data_filled %>%
  mutate(across(X10BNY29042rep1:Cont3rep1, ~ replace_na(.x, 0)))  # Replace NA with 0 in specified columns

# Step 2: Pivot longer
metab_long <- final_data_filled_clean %>%
  pivot_longer(
    cols = X10BNY29042rep1:Cont3rep1,  # Specify the range or individual column names
    names_to = "sample",
    values_to = "value"
  ) %>%
  # Ensure that the 'value' column is numeric and any empty strings are replaced with 0
  mutate(value = as.numeric(value)) %>%
  filter(value != 0) %>%  # Optionally filter out rows where value is zero
  select(-matches("^X(\\.\\d+)?$"))  # Remove columns named X, X.1, ..., X.11

# Step 3: Pivot wider to transform back to wide format
metab_wide <- metab_long %>%
  pivot_wider(
    names_from = sample,
    values_from = value,
    values_fill = list(value = 0)  # Ensure missing values in the new wide format are filled with 0
  )

# List of columns to modify
cols_to_modify <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Convert specified columns to character type
metab_wide[cols_to_modify] <- lapply(metab_wide[cols_to_modify], as.character)

# Replace NA values with empty strings in specified columns
metab_wide[cols_to_modify][is.na(metab_wide[cols_to_modify])] <- ""

# Remove 'X' at the start of column headings
colnames(metab_wide) <- sub("^X", "", colnames(metab_wide))

# Print the first few rows of the resulting dataframe
print(head(metab_wide))

```

# save the filtered filled full dataframe with the PCR replcate seperate
```{r}
# Generate a timestamp
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp (set Miseq run number)
file_name <- paste0(here::here("results/final_data_summaries/MiseqXX_vsearch_summary_filled_filt_PCR rep_"), timestamp, ".csv")

# Save the dataframe with the new file name
write.csv(metab_wide, file = file_name, row.names = FALSE)
```

## There are a number of options for exporting data for further analysis

# OPTION 1: Summarize values by site (with PCR replicates combined)
```{r}
# Summarize the data
summarized_df <- filtered_dataframe %>%
  group_by(site, kingdom, phylum, class, order, family, genus, species, max_p_identity) %>%  # Group by specified columns including max_p_identity
  summarise(reads = sum(reads, na.rm = TRUE), .groups = 'drop') %>%  # Sum the reads for each group
  group_by(site, kingdom, phylum, class, order, family, genus, species) %>%  # Group by the main columns again
  summarise(
    reads = sum(reads, na.rm = TRUE),  # Sum the reads for each group
    max_p_identity = ifelse(all(is.na(max_p_identity)), NA, max(max_p_identity, na.rm = TRUE)),  # Handle all NA case
    .groups = 'drop'
  )

# Collapse rows by kingdom, phylum, class, order, family, genus, species
collapsed_df <- summarized_df %>%
  group_by(site, kingdom, phylum, class, order, family, genus, species) %>%
  summarise(
    reads = sum(reads, na.rm = TRUE),  # Sum the reads for each group
    max_p_identity = ifelse(all(is.na(max_p_identity)), NA, max(max_p_identity, na.rm = TRUE)),  # Get max value of max_p_identity
    .groups = 'drop'
  )

# Pivot wider to transform the site names into columns
wide_summarised_df <- collapsed_df %>%
 pivot_wider(names_from = site, values_from = reads, values_fill = 0)  # Fill missing values with 0

# List of columns to modify
cols_to_modify <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Convert specified columns to character type
wide_summarised_df[cols_to_modify] <- lapply(wide_summarised_df[cols_to_modify], as.character)

# Replace NA values with empty strings in specified columns
wide_summarised_df[cols_to_modify][is.na(wide_summarised_df[cols_to_modify])] <- ""

# Print the resulting wide summarized data frame
print(wide_summarised_df)

```
#Now aggregate species data so that the same species identification do not occur multiple times
```{r}
library(dplyr)

# List of columns to modify
cols_to_modify <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Convert specified columns to character type
wide_summarised_df[cols_to_modify] <- lapply(wide_summarised_df[cols_to_modify], as.character)

# Replace NA values with empty strings in specified columns
wide_summarised_df[cols_to_modify][is.na(wide_summarised_df[cols_to_modify])] <- ""

# Print the resulting wide summarized data frame
print(wide_summarised_df)

# List of columns to group by
group_cols <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Identify the columns that are site_per (excluding group columns and max_p_identity)
site_cols <- setdiff(colnames(wide_summarised_df), c(group_cols, 'max_p_identity'))

# Collapse rows by kingdom, phylum, class, order, family, genus, species
collapsed_wide_summarised_df <- wide_summarised_df %>%
  group_by(across(all_of(group_cols))) %>%
  summarise(
    max_p_identity = max(max_p_identity, na.rm = TRUE),  # Get max value of max_p_identity
    across(all_of(site_cols), function(x) sum(x, na.rm = TRUE)),  # Sum the values for the site_per columns
    .groups = 'drop'
  )

# Print the collapsed data frame
print(collapsed_wide_summarised_df)
```

# save file
```{r}
# save filled dataframe
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp (NOTE:Change the MiSeq number to the dataset you are working with)
file_name1 <- paste0(here::here("results/MiseqXX_final_data_summary_"), timestamp, ".csv")
 
 write.csv(wide_summarised_df, file = file_name1, row.names = FALSE)
```
 
# DATA SUMMARY FOR 10 STIE STUDY (with site_per) 
#```{r}
summarized_per_df <- filtered_dataframe %>%
  group_by(site_per, kingdom, phylum, class, order, family, genus, species, max_p_identity) %>%  # Group by specified columns including max_p_identity
  summarise(reads = sum(reads, na.rm = TRUE), .groups = 'drop') %>%  # Sum the reads for each group
  group_by(site_per, kingdom, phylum, class, order, family, genus, species) %>%  # Group by the main columns again
  summarise(
    reads = sum(reads, na.rm = TRUE),  # Sum the reads for each group
    max_p_identity = ifelse(all(is.na(max_p_identity)), NA, max(max_p_identity, na.rm = TRUE)),  # Handle all NA case
    .groups = 'drop'
  )

# Pivot wider to transform the site names into columns
wide_summarised_per_df <- summarized_per_df %>%
 pivot_wider(names_from = site_per, values_from = reads, values_fill = 0)  # Fill missing values with 0

# List of columns to modify
cols_to_modify <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Convert specified columns to character type
wide_summarised_per_df[cols_to_modify] <- lapply(wide_summarised_per_df[cols_to_modify], as.character)

# Replace NA values with empty strings in specified columns
wide_summarised_per_df[cols_to_modify][is.na(wide_summarised_per_df[cols_to_modify])] <- ""

## Remove 'X' at the start of column headings
colnames(wide_summarised_per_df) <- gsub("^X", "", colnames(wide_summarised_per_df))

print(wide_summarised_per_df)
```
#Now aggregate species data so that the same species identification do not occur multiple times
#```{r}
library(dplyr)

# List of columns to modify
cols_to_modify <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Convert specified columns to character type
wide_summarised_per_df[cols_to_modify] <- lapply(wide_summarised_per_df[cols_to_modify], as.character)

# Replace NA values with empty strings in specified columns
wide_summarised_per_df[cols_to_modify][is.na(wide_summarised_per_df[cols_to_modify])] <- ""

# Print the resulting wide summarized data frame
print(wide_summarised_df)

# List of columns to group by
group_cols <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Identify the columns that are site_per (excluding group columns and max_p_identity)
site_cols <- setdiff(colnames(wide_summarised_per_df), c(group_cols, 'max_p_identity'))

# Collapse rows by kingdom, phylum, class, order, family, genus, species
collapsed_wide_summarised_per_df <- wide_summarised_per_df %>%
  group_by(across(all_of(group_cols))) %>%
  summarise(
    max_p_identity = max(max_p_identity, na.rm = TRUE),  # Get max value of max_p_identity
    across(all_of(site_cols), function(x) sum(x, na.rm = TRUE)),  # Sum the values for the site_per columns
    .groups = 'drop'
  )

# Print the collapsed data frame
print(collapsed_wide_summarised_per_df)
```



#```{r}
# save filled dataframe
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp 
file_name2 <- paste0(here::here("results/Miseq13_final_data_summary_"), timestamp, ".csv")
 
 write.csv(collapsed_wide_summarised_per_df, file = file_name2, row.names = FALSE)
```
