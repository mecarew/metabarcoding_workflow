---
title: "10_site_study_data_filtering"
author: "Melissa Carew"
date: "21/08/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Filling and Filtering notebook for the 10 site study MiSeq13 (this has a different experimental design to other runs) 

This notebook is designed to filling missing taxonomic assignments from the vsearch step (in 03_classification) in the metabarcoding workflow and make corrections to taxonomic assignments that are incorrectly assigned by vsearch due to errors in the DNA barcode reference libraries (SECTION 1).It also performs a final flitering to detect and remove species detections that have most likely arisen from 'mistagging' (SECTION 2). It has been modified to produce a final dataset for the 10 site study.


```{r}
# Load the required libraries
library(dplyr)
library(tidyr)
library(stringr)
library(readxl)
```

# reading in data
The MiSeq13 runs contains the bulk of the data for the 10 site study. However, a few samples require re-running therefore the data need to be retrieved and combined into a final dataset prior to filtering.

Samples requiring re-running
40BNY29042 <- re-run in MiSeq15 (3 replicates 40BNY29042rep3, 40BNY29042rep4 and 40BNY29042rep5 (only need 2 reps))

10FEH41841 <- re-run in MiSeq15 (3 replicates labelled 10FEH41841rep3, 10FEH41841rep4 and 10FEH41841rep5 (only need 2 reps))

30DNG75620 <- re-run in MiSeq16 (3 replicates labelled Elut30DNG79574rep1, Elut1030DNG79574rep2 and Elut30DNG75620rep3 (only need 2 reps))

```{r}
#reset directory 
miseq_folder <- "miseq13"

# Set the directory path for files to be stored on uomshare (NOTE: 'test' will need to be change to your miseq folder name)
uomshare <- paste0("~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/",miseq_folder,"/metabarcoding_workflow")

#read in final vsearch data summaries
miseq13_vsearch_sum <- read.csv(paste0(uomshare, "/final_data_summaries/miseq13_vsearch_data_summary_26_11_2024.csv"))
miseq15_vsearch_sum <-  read.csv("~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/miseq15/metabarcoding_workflow/final_data_summaries/miseq15_vsearch_data_summary_26_11_2024.csv")
# fix a small error in dataframe
miseq15_vsearch_sum <- miseq15_vsearch_sum %>%
  mutate(Consensus = ifelse(asv_code == '956bcb88ea0e4762eefac3c2a93bbb2f', 0.857, Consensus))
miseq16_vsearch_sum <- read.csv("~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/miseq16/metabarcoding_workflow/final_data_summaries/miseq16_vsearch_data_summary_26_11_2024.csv")
```

```{r}
# remove columns to be replaced re-run data from MiSeq13                               
miseq13_vsearch_sum_mod <- miseq13_vsearch_sum[, !(names(miseq13_vsearch_sum) %in% c("X10FEH41843rep1", "X10FEH41843rep2", "X40BNY29042rep1", "X40BNY29042rep2", "X30DNG79574rep1", "X30DNG79574rep2"))]

# subset rerun data from Miseq15 and Miseq16 to merge with miseq13_vsearch_sum_mod
miseq15_vsearch_reruns <- miseq15_vsearch_sum %>% select(asv_code, kingdom, phylum, class, order, family, genus, species, Consensus, max_p_identity, threshold, X40BNY29042rep3, X40BNY29042rep4, X10FEH41841rep3, X10FEH41841rep4, asv_seq, amplicon)
# Rename sample columns to site names
miseq15_vsearch_reruns <- miseq15_vsearch_reruns %>%
  dplyr::rename(X10FEH41843rep3 = X10FEH41841rep3, X10FEH41843rep4 = X10FEH41841rep4)

miseq16_vsearch_reruns <- miseq16_vsearch_sum %>% select(asv_code, kingdom, phylum, class, order, family, genus, species, Consensus, max_p_identity, threshold, Elut30DNG79574rep1, Elut30DNG79574rep2, asv_seq, amplicon)
# rename sample name to site name
miseq16_vsearch_reruns <- miseq16_vsearch_reruns %>% dplyr::rename(`X30DNG79574rep1` = Elut30DNG79574rep1, `X30DNG79574rep2` = Elut30DNG79574rep2)

# merge all data into a final vsearch data frame for the 10 site study
# Pivot data so it can be bound
miseq13_vsearch_sum_mod_long <- miseq13_vsearch_sum_mod %>%
  pivot_longer(
    cols = X10BNY29042rep1:Cont3rep1,  # Specify the range or individual column names
    names_to = "sample",
    values_to = "value"
  ) %>%
  # Ensure that the 'value' column is numeric and any empty strings are replaced with 0
  mutate(value = as.numeric(value)) %>%
  filter(value != 0) %>%  # Optionally filter out rows where value is zero
  select(-matches("^X(\\.\\d+)?$"))  # Remove columns named X, X.1, ..., X.11

miseq15_vsearch_long <- miseq15_vsearch_reruns %>%
  pivot_longer(
    cols = X40BNY29042rep3:X10FEH41843rep4,  # Specify the range or individual column names
    names_to = "sample",
    values_to = "value"
  ) %>%
  # Ensure that the 'value' column is numeric and any empty strings are replaced with 0
  mutate(value = as.numeric(value)) %>%
  filter(value != 0) %>%  # Optionally filter out rows where value is zero
  select(-matches("^X(\\.\\d+)?$"))  # Remove columns named X, X.1, ..., X.11
 
miseq16_vsearch_long <- miseq16_vsearch_reruns %>%
  pivot_longer(
    cols = X30DNG79574rep1:X30DNG79574rep2,  # Specify the range or individual column names
    names_to = "sample",
    values_to = "value"
  ) %>%
  # Ensure that the 'value' column is numeric and any empty strings are replaced with 0
  mutate(value = as.numeric(value)) %>%
  filter(value != 0) %>%  # Optionally filter out rows where value is zero
  select(-matches("^X(\\.\\d+)?$"))  # Remove columns named X, X.1, ..., X.11

#bind datasets back together
final_10sites_long <- rbind(miseq13_vsearch_sum_mod_long, miseq15_vsearch_long, miseq16_vsearch_long)

# final_10sites_long <- final_10sites_long %>%
#   mutate(sample = str_trim(sample))

final_df  <- final_10sites_long  %>%
  pivot_wider(
    names_from = sample,
    values_from = value,
    values_fill = 0  # optionally fill missing sample-asv combos with 0
  )
```



## SECTION 1: Filling in missing/incorrect taxonomic classifications

A file based on analysis of Miseq datasets was constructed ('asv_library_corrected_Jul_25.csv'). This file captures ASVs (amplicon sequence variants) from the DADA2 step (where the metabarcoding data is reduced into a series of sequence groups with high sequence similarity i.e. ASVs) and this file:
-   fills truncated taxonomy assignments from the vsearch step
-   updates/corrects any missing identifications or errors in classification
-   identifies any ambiguous taxonomic assignments that should be considered as groups

```{r}
# read in updated asv library
asv_lib <- read.csv("~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/asv_source_files_MC/asv_library_corrected_Jul_25.csv")
```

```{r}
# Step 1: Select only taxonomy and match fields from asv_lib
asv_lib_subset <- asv_lib %>%
  select(asv_code, 
         kingdom_new = kingdom, phylum_new = phylum, class_new = class, 
         order_new = order, family_new = family, genus_new = genus, 
         species_new = species, max_p_identity_new = max_p_identity)

# Step 2: Join to final_df and update taxonomy fields ONLY
final_df_updated <- final_df %>%
  left_join(asv_lib_subset, by = "asv_code") %>%
  mutate(
    kingdom        = coalesce(kingdom_new, kingdom),
    phylum         = coalesce(phylum_new, phylum),
    class          = coalesce(class_new, class),
    order          = coalesce(order_new, order),
    family         = coalesce(family_new, family),
    genus          = coalesce(genus_new, genus),
    species        = coalesce(species_new, species),
    max_p_identity = coalesce(max_p_identity_new, max_p_identity)
  ) %>%
  select(-ends_with("_new"))  # remove temporary columns

# Step 6: Recombine with the rest of the final_df that had no update
#final_updated_df <- bind_rows(df1_updated, df3)

# Step 7: Reorder columns if needed
final_df_filled <- final_df_updated %>%
  select(asv_code, kingdom, phylum, class, order, family, genus, species, max_p_identity, everything(), asv_seq, amplicon)
```

```{r}
# Update 'species' column to only include names if max threshold is above 97
final_df_filled <-  final_df_filled %>%
  dplyr::mutate(species = if_else(max_p_identity < 97, "", species))

# Update 'genus' column to only include names if max threshold is above 95
final_df_filled <-  final_df_filled %>%
  dplyr::mutate(genus = if_else(max_p_identity < 95, "", genus))

# Update 'family' column to only include names if max threshold is above 90
final_df_filled <-  final_df_filled %>%
  dplyr::mutate(family = if_else(max_p_identity < 90, "", family))

# Update 'order' column to only include names if max threshold is above 80
final_df_filled <-  final_df_filled %>%
  dplyr::mutate(order = if_else(max_p_identity < 85, "",order))
```


```{r}
dir.create(paste0(uomshare, "/ten_site_data_summaries/"))

# Save filled dataframe
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp
file_name <- paste0(uomshare, "/ten_site_data_summaries/ten_sites_vsearch_data_summary_filled_", timestamp, ".csv")

# Save the dataframe with the new file name
write.csv(final_df_filled, file = file_name, row.names = FALSE)
```
______________________________

## SECTION 2: PCR filtering
This section is designed to format data and filter it for 'mistagging'. Mistagging or 'tag switching' can occur causing ASVs to be assigned to the wrong sample. This occurs at a low frequency and is usually random. PCR replicates included in the study design help detect and remove these errors. We use three PCR replicates and filter out any species/ASV detections that occur in only one PCR replicate (which are most likely from mistagging). We also add a second flitering step for the five most common ASV (those with the highest number of reads/sequences). These are more likely to be involved in mistagging and can appear in multiple PCR replicate we add a reads threshold for removing these detections.

# Change data to long formmat (list) for PCR replicate filtering 
Note: This code will need to be configured for each dataset, so that the rows containing the read counts are specified (e.g. pivot_longer(cols = X10BNY29042rep1:Cont3rep1), the 'X10BNY29042rep1:Cont3rep1' will need to be changed to the first and last column name with the reads data for each dataset
X10WMI2320rep2:X30DNG318813rep2
```{r}
# Step 1: Add 'X' prefix to column names if they start with a number
final_df_filled <- final_df_filled %>%
  rename_with(~ str_replace_all(., "^([0-9])", "X\\1"))

# Step 2: Pivot longer to transform selected columns into rows
metab_long <- final_df_filled %>%
  pivot_longer(cols = X10WMI2320rep2:X30DNG318813rep2, names_to = "sample", values_to = "value") %>%
  filter(value != 0) %>%  # Remove rows where value is zero
  select(-matches("^X(\\.\\d+)?$"))  # Remove unwanted columns named X, X.1, etc.

# Step 3: Create a 'replicate' column by extracting last character if it ends in 'rep' followed by a number
metab_long <- metab_long %>%
  mutate(replicate = ifelse(grepl("rep[123456]$", sample), str_extract(sample, "rep[123456]$"), ""))

# Step 4: Extract the 'site_per' and 'per' columns based on 'sample'
metab_long <- metab_long %>%
  mutate(site_per = sub("(rep[123456])$", "", sample),  # Remove replicate suffix from sample to get site_per
         per = substr(site_per, 2, 3))  # Take the first two characters of site_per for 'per' column

# Remove the "X" prefix from 'sample' and 'site_per' columns
metab_long <- metab_long %>%
  mutate(
    sample = str_remove(sample, "^X"),
    site_per = str_remove(site_per, "^X")
  )

# Step 5: Create a 'site' column by further trimming 'site_per'
metab_long <- metab_long %>%
  mutate(site = sub("^\\d{2}", "", site_per))  # Remove initial numeric prefix from site_per for site

# Step 6: Reorder and rename columns as needed
metab_long <- metab_long %>%
  select(sample, replicate, site_per, per, site, everything()) %>%  # Adjust column order
  rename(reads = value)  # Rename 'value' to 'reads'

# Print the transformed data frame
print(metab_long)
```
# remove low frequency detections < 5 reads
```{r}
metab_long_hf <- metab_long %>%
  filter(reads >= 5)

metab_long_lf <- metab_long %>%
  filter(reads < 5)  # Corrected this line

# Verify the number of records before and after filtering
original_count <- nrow(metab_long)
hf_count <- nrow(metab_long_hf)
lf_count <- nrow(metab_long_lf)

cat("Original number of records:", original_count, "\n")
cat("Number of records with reads >= 5:", hf_count, "\n")
cat("Number of records with reads < 5:", lf_count, "\n")
```

# Filtering single PCR replicate detections

# filtering by 'species' subset out data with species ids
```{r}
# Subset where 'species' has non-empty character values
metab_long_with_species <- metab_long_hf %>% filter(!is.na(species) & species != "")

# Subset where 'species' has empty cells or NA (for ASV filtering)
metab_long_empty_species <- metab_long_hf %>% filter(is.na(species) | species == "")
```

# filter based on common species as these are more likely to be invovled in misstagging in the dataset
```{r}
# Step 1: Group by species and summarize the total reads
species_reads <- metab_long_with_species %>%
  group_by(species) %>%
  summarise(total_reads = sum(reads, na.rm = TRUE)) %>%
  arrange(desc(total_reads))

# Print species_reads to verify the summarized data
print(species_reads)

# Step 2: Extract the top 10 species detections based on 'total_reads' values
top_species <- species_reads %>%
  top_n(10, wt = total_reads) %>%
  pull(species)

# Print top_species to verify the top species
print(top_species)

# Step 3: Filter out reads below 10 for the top 10 species in filtered_dataframe
filtered_dataframe_top_sp <- metab_long_with_species %>%
  filter(!(species %in% top_species & reads < 50))

filtered_dataframe_top_sp_rem <- metab_long_with_species %>%
  filter((species %in% top_species & reads < 50))

# Verify the number of records before and after filtering
original_count <- nrow(metab_long_with_species)
filtered_count <- nrow(filtered_dataframe_top_sp)

cat("Original number of records:", original_count, "\n")
cat("Number of records after filtering:", filtered_count, "\n")
cat("Number of records removed:", original_count - filtered_count, "\n")
```

# remove species occurences that don't occur in more than two replicate per site
The study desigh has two PCR replicates per percentage (10, 20, 30, 40). A conservative replicate filtering approach has been applied a species/ASV will only be retained if it appears in more that two PCR replicates per site (across all replicates and percentages)
```{r}
# Remove the count column as it is no longer needed
# Step 1: Create the counts data frame
counts_sp <- filtered_dataframe_top_sp %>%
  group_by(site, species) %>%
  summarise(count = n(), .groups = 'drop')

# Step 2: Filter out rows where the combination of 'site' and 'species' appears only once
sp_fil_metab_long_with_species <- filtered_dataframe_top_sp %>%
  left_join(counts_sp, by = c("site", "species")) %>%
  filter(count > 2) %>%
  select(-count)  # Remove the count column as it is no longer needed

# Check samples removed from dataframe
sp_fil_metab_long_rem <- filtered_dataframe_top_sp %>% filter((species %in% counts_sp$species[counts_sp$count == 1] & site %in% counts_sp$site[counts_sp$count == 2]))

# Verify the number of records before and after filtering
original_count <- nrow(filtered_dataframe_top_sp)
filtered_count <- nrow(sp_fil_metab_long_with_species)

cat("Original number of records:", original_count, "\n")
cat("Number of records after filtering:", filtered_count, "\n")
cat("Number of records removed:", original_count - filtered_count, "\n")
```

# remove single ASV occurences
Remove ASV's with only a single occurrence in a sample (in only one PCR replicate) this targets ASV's without identifications or OTU groupings
```{r}
# Step 1: Create the counts_asv data frame
counts_asv <- metab_long_empty_species %>%
  group_by(asv_code, site) %>%
  summarise(count = n(), .groups = 'drop')

# Inspect the counts_asv data frame
print(counts_asv)

# Step 2: Join and filter based on counts
asv_fil_metab_long <- metab_long_empty_species %>%
  left_join(counts_asv, by = c("asv_code", "site")) %>%
  filter(count > 2) %>%
  select(-count)  # Remove the count column as it is no longer needed

# Step 3: Verify the number of records before and after filtering
original_count <- nrow(metab_long_empty_species)
filtered_count <- nrow(asv_fil_metab_long)

cat("Original number of records:", original_count, "\n")
cat("Number of records after filtering:", filtered_count, "\n")
cat("Number of records removed:", original_count - filtered_count, "\n")
```

# bind filtered and filled dataframes
```{r}
#rejoin_dataframes
filtered_dataframe <- rbind(sp_fil_metab_long_with_species, asv_fil_metab_long)

# Define the values to be removed
values_to_remove <- c('DrosX', 'Dros', 'MAR2T','MART2', 'Cont')

# Create the new data frame with the rows to be removed
controls_long <- filtered_dataframe[filtered_dataframe$site %in% values_to_remove, ]

# Update the original data frame to keep only the remaining rows
filtered_dataframe_samples <- filtered_dataframe[!filtered_dataframe$site %in% values_to_remove, ]

filtered_dataframe_samples <- filtered_dataframe_samples %>%
  mutate(
    sample = gsub("^X", "", sample),      # Remove 'X' at the beginning of 'sample'
    site_per = gsub("^X", "", site_per)   # Remove 'X' at the beginning of 'site_per'
  )
# remove 'NA' from dataframe
filtered_dataframe_samples <- filtered_dataframe_samples %>%
  mutate(across(c(kingdom, phylum, class, order, family, genus, species), ~ replace_na(., "")))

```

# check if 'CONTROL SPIKE' is in samples
```{r}
if ("CONTROL SPIKE" %in% filtered_dataframe_samples$species) {
  print("CONTROL SPIKE found")
} else {
  print("No CONTROL SPIKE found")
}
```

# save data for modeling in long format (includes PCR replicates)
```{r}
# Generate a timestamp
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp
file_name <- paste0(uomshare, "/ten_site_data_summaries/data_for_analysis/ten_sites_long_format_", timestamp, ".csv")
# Save the dataframe with the new file name
write.csv(filtered_dataframe_samples, file = file_name, row.names = FALSE)
```

# OPTION 1: Summarize values by site (with PCR replicates combined)
```{r}
# Summarize the data
summarized_df <- filtered_dataframe_samples %>%
  group_by(site_per, kingdom, phylum, class, order, family, genus, species, max_p_identity) %>%  # Group by specified columns including max_p_identity
  summarise(reads = sum(reads, na.rm = TRUE), .groups = 'drop') %>%  # Sum the reads for each group
  group_by(site_per, kingdom, phylum, class, order, family, genus, species) %>%  # Group by the main columns again
  summarise(
    reads = sum(reads, na.rm = TRUE),  # Sum the reads for each group
    max_p_identity = ifelse(all(is.na(max_p_identity)), NA, max(max_p_identity, na.rm = TRUE)),  # Handle all NA case
    .groups = 'drop'
  )

# Collapse rows by kingdom, phylum, class, order, family, genus, species
collapsed_df <- summarized_df %>%
  group_by(site_per, kingdom, phylum, class, order, family, genus, species) %>%
  summarise(
    reads = sum(reads, na.rm = TRUE),  # Sum the reads for each group
    max_p_identity = ifelse(all(is.na(max_p_identity)), NA, max(max_p_identity, na.rm = TRUE)),  # Get max value of max_p_identity
    .groups = 'drop'
  )

# Pivot wider to transform the site_per names into columns
wide_summarised_df <- collapsed_df %>%
 pivot_wider(names_from = site_per, values_from = reads, values_fill = 0)  # Fill missing values with 0

# List of columns to modify
cols_to_modify <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Convert specified columns to character type
wide_summarised_df[cols_to_modify] <- lapply(wide_summarised_df[cols_to_modify], as.character)

# Replace NA values with empty strings in specified columns
wide_summarised_df[cols_to_modify][is.na(wide_summarised_df[cols_to_modify])] <- ""

# Print the resulting wide summarized data frame
print(wide_summarised_df)

```

#Now aggregate species data so that the same species identification do not occur multiple times
```{r}
library(dplyr)

# List of columns to modify
cols_to_modify <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Convert specified columns to character type
wide_summarised_df[cols_to_modify] <- lapply(wide_summarised_df[cols_to_modify], as.character)

# Replace NA values with empty strings in specified columns
wide_summarised_df[cols_to_modify][is.na(wide_summarised_df[cols_to_modify])] <- ""

# Print the resulting wide summarized data frame
print(wide_summarised_df)

# List of columns to group by
group_cols <- c('kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species')

# Identify the columns that are site_per_per (excluding group columns and max_p_identity)
site_per_cols <- setdiff(colnames(wide_summarised_df), c(group_cols, 'max_p_identity'))

# Collapse rows by kingdom, phylum, class, order, family, genus, species
collapsed_wide_summarised_df <- wide_summarised_df %>%
  group_by(across(all_of(group_cols))) %>%
  summarise(
    max_p_identity = max(max_p_identity, na.rm = TRUE),  # Get max value of max_p_identity
    across(all_of(site_per_cols), function(x) sum(x, na.rm = TRUE)),  # Sum the values for the site_per columns
    .groups = 'drop'
  )

# Print the collapsed data frame
print(collapsed_wide_summarised_df)
```

# save file
```{r}
# save filled dataframe
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp (NOTE:Change the MiSeq number to the dataset you are working with)
file_name1 <- paste0(uomshare,"/ten_site_data_summaries/data_for_analysis/ten_sites_wide_format_", timestamp, ".csv") 

 write.csv(collapsed_wide_summarised_df, file = file_name1, row.names = FALSE)
```

