---
title: "Adding_additonal_taxonomic_information and replicate filtering"
author: "Melissa Carew"
date: "06/06/2024"
output: html_document
---

# Filling and Filtering notebook for all Miseq runs except MiSeq13 (this has a different experimental design)

This notebook is designed to filling missing taxonomic assignments from the vsearch step (in 03_classification) in the metabarcoding workflow and make corrections to taxonomic assignments that are incorrectly assigned by vsearch due to errors in the DNA barcode reference libraries (SECTION 1).It also performs a final flitering to detect and remove species detections that have most likely arisen from 'mistagging' (SECTION 2)

```{r}
# Load the required libraries
library(dplyr)
library(tidyr)
library(stringr)
library(readxl)
library(here)
```

## SECTION 1: Filling in missing/incorrect taxonomic classifications

A file based on analysis of Miseq datasets was constructed ('asv_library_97_aug24.xlsx'). This file captures ASVs (amplicon sequence variants) from the DADA2 step (where the metabarcoding data is reduced into a series of sequence groups with high sequence similarity i.e. ASVs) and this file:
-   fills truncated taxonomy assignments from the vsearch step (sheet 1)
-   groups unclassified sequences into 'OTUs' (sheet 2)
-   updates/corrects any missing identifications or errors in classification (sheet 3)
-   identifies any ambiguous taxonomic assignments that should be considered as complexes (sheet 4)

```{r setup, warning=FALSE}
# read in flies

file_path <- "~/git/metabarcoding_workflow/data/asv_library_corrected.xlsx"

# Get the names of the sheets
sheet_names <- readxl::excel_sheets(file_path)

# Read the sheets into as dataframes
truncated_fills_df <- readxl::read_excel(file_path, sheet = sheet_names[1])  # contains full taxonomic information for classifications with truncated taxonomy
#otu_groups_df <- readxl::read_excel(file_path, sheet = sheet_names[2])# groups unclassified sequences into 'OTUs'
max_p_update_df <- readxl::read_excel(file_path, sheet = sheet_names[2])# updates/corrects any missing identifications or errors in classification (change sheet to 2 as not using OTU grouping)
#complexes_df <- readxl::read_excel(file_path, sheet = sheet_names[4])# identifies any ambiguous taxonomic assignments that should be considered as complexes

# Replace NA values with empty strings
truncated_fills_df <- truncated_fills_df %>% mutate_all(~ ifelse(is.na(.), "", .))
#otu_groups_df <- otu_groups_df %>% mutate_all(~ ifelse(is.na(.), "", .))
max_p_update_df <- max_p_update_df %>% mutate_all(~ ifelse(is.na(.), "", .))

#read in raw dataframe (if starting with a clear global environment)

miseq_folder <- "miseq20"

uomshare <- paste0("~/uomShare/wergStaff/MelCarew/git-data/metabarcoding_workflow/",miseq_folder,"/metabarcoding_workflow")
#uomshare <- paste0("~/uomShare/wergStaff/MelCarew/git-data/",miseq_folder,"/metabarcoding_workflow")
final_df <- read.csv(paste0(uomshare, "/final_data_summaries/", miseq_folder, "_vsearch_data_summary_26_11_2024.csv")) # change the Miseq run number and time stamp
```

# remove identifications below thresholds for species species, genus, family and order
```{r}
# Update 'species' column to only include names if max threshold is above 97
final_df <-  final_df %>%
  dplyr::mutate(species = if_else(max_p_identity < 97, "", species))

# Update 'genus' column to only include names if max threshold is above 95
final_df <-  final_df %>%
  dplyr::mutate(genus = if_else(max_p_identity < 95, "", genus))

# Update 'family' column to only include names if max threshold is above 90
final_df <-  final_df %>%
  dplyr::mutate(family = if_else(max_p_identity < 90, "", family))

# Update 'order' column to only include names if max threshold is above 80
final_df <-  final_df %>%
  dplyr::mutate(order = if_else(max_p_identity < 85, "",order))
```


# Fill in truncated taxonomic assigments (using sheet 1: truncated_fills_df)
```{r}
# Find common IDs
common_ids <- intersect(truncated_fills_df$asv_code, final_df$asv_code)

print(common_ids)

df1 <- final_df[final_df$asv_code %in% common_ids, ] # asvs in common in final_df
df2 <- truncated_fills_df[truncated_fills_df$asv_code %in% common_ids, ]  # asvs in common in truncated_fills_df
df3 <- final_df[!final_df$asv_code %in% common_ids, ] # left data in final dataframe


# Update the 'species' value in df1 based on matching 'asv_code'
for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$species[i] <- df2$species[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$genus[i] <- df2$genus[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$family[i] <- df2$family[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$order[i] <- df2$order[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$class[i] <- df2$class[df2$asv_code == target_asv_code]
}

for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$phylum[i] <- df2$phylum[df2$asv_code == target_asv_code]
}
 
for (i in 1:nrow(df1)) {
  target_asv_code <- df1$asv_code[i]
  df1$kingdom[i] <- df2$kingdom[df2$asv_code == target_asv_code]
}
print(df1)
```

# Add 'OTU' groupings from 'Geneious prime' denovo analysis
Any unclassified ASVs have been grouped together at a similarity threshold of <3%. These ASVs are denoted with the name 'sp. MC-ARCO'. 
```{r}
# # Find common IDs
# common_ids2 <- intersect(otu_groups_df$asv_code, df3$asv_code)
# 
# df4 <- df3[df3$asv_code %in% common_ids2, ] # asvs in common in remaining final_df
# df5 <- otu_groups_df[otu_groups_df$asv_code %in% common_ids2, ] # asvs in common in truncated_fills_df
# df6 <- df3[!df3$asv_code %in% common_ids2, ] #  data left in final dataframe
# 
# # Update the 'species' value in df1 based on matching 'asv_code'
# for (i in 1:nrow(df4)) {
#   target_asv_code <- df4$asv_code[i]
#   df4$species[i] <- df5$species[df5$asv_code == target_asv_code]
# }
# 
# for (i in 1:nrow(df4)) {
#   target_asv_code <- df4$asv_code[i]
#   df4$genus[i] <- df5$genus[df5$asv_code == target_asv_code]
# }
# 
# for (i in 1:nrow(df4)) {
#   target_asv_code <- df4$asv_code[i]
#   df4$family[i] <- df5$family[df5$asv_code == target_asv_code]
# }
# 
# for (i in 1:nrow(df4)) {
#   target_asv_code <- df4$asv_code[i]
#   df4$order[i] <- df5$order[df5$asv_code == target_asv_code]
# }
# 
# for (i in 1:nrow(df4)) {
#   target_asv_code <- df4$asv_code[i]
#   df4$class[i] <- df5$class[df5$asv_code == target_asv_code]
# }
# 
# for (i in 1:nrow(df4)) {
#   target_asv_code <- df4$asv_code[i]
#   df4$phylum[i] <- df5$phylum[df5$asv_code == target_asv_code]
# }
#  
# for (i in 1:nrow(df4)) {
#   target_asv_code <- df4$asv_code[i]
#   df4$kingdom[i] <- df5$kingdom[df5$asv_code == target_asv_code]
# }
# 
# print(df4)
```
# Add in identification data for taxa missing from reference database (sheet 3)
```{r}
# Find common IDs
common_ids3 <- intersect(max_p_update_df$asv_code, final_df$asv_code)

# commented out as not using OTU groups yet.
# df7 <- df6[df6$asv_code %in% common_ids3, ] # asvs in common in final_df
# df8 <- max_p_update_df[max_p_update_df$asv_code %in% common_ids3, ] # asv requiring updating
# df9 <- df6[!df6$asv_code %in% common_ids3, ] # data left in final dataframe

# code adjusted to skip OTU groupings
df7 <- df3[df3$asv_code %in% common_ids3, ] # asvs in common in final_df
df8 <- max_p_update_df[max_p_update_df$asv_code %in% common_ids3, ] # asv requiring updating
df9 <- df3[!df3$asv_code %in% common_ids3, ] # data left in final dataframe

# Update the 'species' value in df1 based on matching 'asv_code'
for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$species[i] <- df8$species[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$genus[i] <- df8$genus[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$family[i] <- df8$family[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$order[i] <- df8$order[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$class[i] <- df8$class[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$phylum[i] <- df8$phylum[df8$asv_code == target_asv_code]
}
 
for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$kingdom[i] <- df8$kingdom[df8$asv_code == target_asv_code]
}

for (i in 1:nrow(df7)) {
  target_asv_code <- df7$asv_code[i]
  df7$max_p_identity[i] <- df8$max_p_identity[df8$asv_code == target_asv_code]
}

print(df7)
```
# combine all filled dataframe into a single dataframe
```{r}
# bind data back together
#final_data_filled <- rbind(df1, df4, df7, df9)
 
# bind with OTU grouping
final_data_filled <- rbind(df1, df7, df9)
# note the "final_data_filled" and "final_df" should be the same size 

# Check if there are any duplicates in the asv_code column (there should NOT be)
duplicates <- final_data_filled %>%
  filter(duplicated(asv_code) | duplicated(asv_code, fromLast = TRUE))

# Print the duplicate rows
#print(duplicates) # You should not get a table

# Check the final vsearch and filled dataframe have the same number of rows 
if (nrow(final_df) == nrow(final_data_filled)) {
  cat("The dataframes have the same number of rows. Proceed \n")
} else {
  cat("The data frames have a different number of rows. Check \n")
}
```

# check 'final_filled_df' for any species complexes and add this information (from sheet 4)
```{r}
# # Use left_join to match and then mutate to replace the species with complex
# final_data_filled <- final_data_filled %>%
#   left_join(complexes_df, by = c("species" = "bin_name")) %>%
#   mutate(species = ifelse(!is.na(complex), complex, species)) %>%
#   select(-complex)
# 
# # Print the modified dataframe
# print(final_data_filled)
```

# adjust Cont names for flitering so that controls have the same names
```{r message=FALSE, warning=FALSE}
# List of old and new column names
old_names <- c("Cont1", "cont1", "COICont1" ,"Cont2", "cont2", "COICont2", "Cont3", "COICont3", "Cont4", "Cont5", "Cont6", "Cont7", "Cont8")
new_names <- c("Contrep1", "Contrep1", "Contrep1", "Contrep2", "Contrep2","Contrep2", "Contrep3", "Contrep3" ,"Contrep4", "Contrep5", "Contrep6", "Contrep7", "Contrep8")

# note some controls will not exist in some datasets

# Renaming only the columns that exist in the dataframe
final_data_filled <- final_data_filled %>%
  rename_at(vars(one_of(old_names)), ~ new_names[match(., old_names)])

# Print the modified dataframe
print(final_data_filled)
```

```{r}
# Save filled dataframe
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp (NOTE: Change the MiSeq number to the dataset you are working with)
file_name <- paste0(uomshare, "/final_data_summaries/", miseq_folder, "_vsearch_data_summary_filled_", timestamp, ".csv")

# Save the dataframe with the new file name
write.csv(final_data_filled, file = file_name, row.names = FALSE)
```
______________________________

## SECTION 2: PCR filtering
This section is designed to format data and filter it for 'mistagging'. Mistagging or 'tag switching' can occur causing ASVs to be assigned to the wrong sample. This occurs at a low frequency and is usually random. PCR replicates included in the study design help detect and remove these errors. We use three PCR replicates and filter out any species/ASV detections that occur in only one PCR replicate (which are most likely from mistagging). We also add a second flitering step for the five most common ASV (those with the highest number of reads/sequences). These are more likely to be involved in mistagging and can appear in multiple PCR replicate we add a reads threshold for removing these detections.

# Change data to long formmat (list) for PCR replicate filtering 
Note: This code will need to be configured for each dataset, so that the rows containing the read counts are specified (e.g. pivot_longer(cols = X10BNY29042rep1:Cont3rep1), the 'X10BNY29042rep1:Cont3rep1' will need to be changed to the first and last column name with the reads data for each dataset
Miseq13:X10BNY29042rep1:Cont3rep1 
Miseq15:X10FEH41841rep3:S21YAR36377rep3
Miseq16:Contrep1:S21WAY30960rep3
Miseq17:A22BNY29042rep1:S21YZJ1175rep6
Miseq18:A22ALD26081rep1:S22YZ8202rep3
Miseq19:A22BF3373rep1:S22YPB224rep3
Miseq20:A23BNY29042rep1:S22SPL263rep6
```{r}
# Modify column names to prefix 'X' if they start with a number
#final_data_filled <- final_data_filled %>%
  #rename_with(~ str_replace_all(., "^([0-9])", "X\\1"))

# Pivot longer to transform columns 13 to 40 into rows 
metab_long <- final_data_filled %>%
  tidyr::pivot_longer(cols = A23BNY29042rep1:S22SPL263rep6, names_to = "sample", values_to = "value") %>%
  filter(value != 0) %>%  # Filter out rows where value is zero
  select(-matches("^X(\\.\\d+)?$"))  # Remove columns named X, X.1, ..., X.11

# Reorder columns with variable as the first column
metab_long <- metab_long %>%
  select(sample, everything())  # Move variable column to the first position

# Create a new column "replace" by pasting the final character if string ends in 'rep1', 'rep2', or 'rep3'
metab_long <- metab_long %>%
  mutate(replicate = ifelse(grepl("rep[12345678]$", sample), paste0(substr(sample, nchar(sample), nchar(sample)), ""), ""))

# Move the "replace" column to be the second column
metab_long <- metab_long %>%
  select(sample, replicate, everything())  # Move "replace" to the second column

#metab_long <- metab_long %>%
#  mutate(sample = str_replace(sample, "^X", ""))

# add 'factor' column
metab_long$site_per <- substr(metab_long$sample, 1, nchar(metab_long$sample))

# remove replicate data
metab_long <- metab_long %>%
   dplyr::mutate(site_per = sub("(rep1|rep2|rep3|rep4|rep5|rep6|rep7|rep8)$", "", site_per))

# Move the "replace" column to be the second column
metab_long <- metab_long %>%
  select(sample, replicate, site_per, everything())  # Move "replace" to the second column

# add 'factor' column
metab_long$site <- substr(metab_long$site_per, 1, nchar(metab_long$site_per))

# remove replicate data (for Miseq13 only)
#metab_long <- metab_long %>% dplyr::mutate(site = sub("(X10|X20|X30|X40|)", "", site))

#Move the "replace" column to be the second column (for Miseq13 only)
#metab_long <- metab_long %>%
#select(sample, replicate, site_per, site, everything())  # Move "replace" to the second column

# Move the "replace" column to be the second column
metab_long <- metab_long %>%
  select(sample, replicate, site, everything())  # Move "replace" to the second column

# Rename the 'values' column to 'reads' using dplyr's rename()
metab_long <- metab_long %>% dplyr::rename(`reads` = value)

# remove site percentage column
metab_long <- metab_long %>%
  select(-site_per)
```

# remove low frequency detections < 5 reads
```{r}
metab_long_hf <- metab_long %>%
  filter(reads >= 5)

metab_long_lf <- metab_long %>%
  filter(reads < 5)  # Corrected this line

# Verify the number of records before and after filtering
original_count <- nrow(metab_long)
hf_count <- nrow(metab_long_hf)
lf_count <- nrow(metab_long_lf)

cat("Original number of records:", original_count, "\n")
cat("Number of records with reads >= 5:", hf_count, "\n")
cat("Number of records with reads < 5:", lf_count, "\n")
```

# Filtering single PCR replicate detections

# filtering by 'species' subset out data with species ids
```{r}
# Subset where 'species' has non-empty character values
metab_long_with_species <- metab_long_hf %>% filter(!is.na(species) & species != "")

# Subset where 'species' has empty cells or NA (for ASV filtering)
metab_long_empty_species <- metab_long_hf %>% filter(is.na(species) | species == "")
```

# filter based on common species as these are more likely to be invovled in misstagging in the dataset
```{r}
# Step 1: Group by species and summarize the total reads
species_reads <- metab_long_with_species %>%
  group_by(species) %>%
  summarise(total_reads = sum(reads, na.rm = TRUE)) %>%
  arrange(desc(total_reads))

# Print species_reads to verify the summarized data
print(species_reads)

# Step 2: Extract the top 10 species detections based on 'total_reads' values
top_species <- species_reads %>%
  top_n(10, wt = total_reads) %>%
  pull(species)

# Print top_species to verify the top species
print(top_species)

# Step 3: Filter out reads below 50 for the top 10 species in filtered_dataframe
filtered_dataframe_top_sp <- metab_long_with_species %>%
  filter(!(species %in% top_species & reads < 50))

filtered_dataframe_top_sp_rem <- metab_long_with_species %>%
  filter((species %in% top_species & reads < 50))

# Verify the number of records before and after filtering
original_count <- nrow(metab_long_with_species)
filtered_count <- nrow(filtered_dataframe_top_sp)

cat("Original number of records:", original_count, "\n")
cat("Number of records after filtering:", filtered_count, "\n")
cat("Number of records removed:", original_count - filtered_count, "\n")
```


# remove single species occurences (species detection that occur in only one PCR replicate)
```{r}
# Remove the count column as it is no longer needed
# Step 1: Create the counts data frame
counts_sp <- filtered_dataframe_top_sp %>%
  group_by(site, species) %>%
  summarise(rep_count = n_distinct(replicate), .groups = 'drop')  # Count distinct replicates

# Step 2: Filter out rows where the combination of 'site' and 'species' appears only once
sp_fil_metab_long_with_species <- filtered_dataframe_top_sp %>%
  left_join(counts_sp, by = c("site", "species")) %>%
filter(rep_count > 1) %>%
  select(-rep_count)  # Remove the rep_count column as it is no longer needed

# Check samples removed from dataframe
# sp_fil_metab_long_rem <- filtered_dataframe_top_sp %>% filter((species %in% counts_sp$species[counts_sp$count == 1] & site %in% counts_sp$site[counts_sp$count == 1]))

sp_fil_metab_long_rem <- filtered_dataframe_top_sp %>%
  filter(species %in% counts_sp$species[counts_sp$rep_count == 1] &
           site %in% counts_sp$site[counts_sp$rep_count == 1])

# Verify the number of records before and after filtering
original_count <- nrow(filtered_dataframe_top_sp)
filtered_count <- nrow(sp_fil_metab_long_with_species)

cat("Original number of records:", original_count, "\n")
cat("Number of records after filtering:", filtered_count, "\n")
cat("Number of records removed:", original_count - filtered_count, "\n")

# Print the modified dataframe
print(sp_fil_metab_long_with_species)

```

# remove single ASV occurences
Remove ASV's with only a single occurrence in a sample (in only one PCR replicate) this targets ASV's without identifications or OTU groupings
```{r}
# Step 1: Create the counts_asv data frame
counts_asv <- metab_long_empty_species %>%
  group_by(asv_code, site) %>%
  summarise(rep_count = n_distinct(replicate), .groups = 'drop')  # Count distinct replicates

# Inspect the counts_asv data frame
print(counts_asv)

# Step 2: Join and filter based on counts
asv_fil_metab_long <- metab_long_empty_species %>%
  left_join(counts_asv, by = c("asv_code", "site")) %>%
filter(rep_count > 1) %>%
  select(-rep_count)  # Remove the rep_count column as it is no longer needed
# Step 3: Verify the number of records before and after filtering
original_count <- nrow(metab_long_empty_species)
filtered_count <- nrow(asv_fil_metab_long)

cat("Original number of records:", original_count, "\n")
cat("Number of records after filtering:", filtered_count, "\n")
cat("Number of records removed:", original_count - filtered_count, "\n")
```
# bind filtered and filled dataframes
```{r}
#rejoin_dataframes
filtered_dataframe <- rbind(sp_fil_metab_long_with_species, asv_fil_metab_long)

# Define the values to be removed
values_to_remove <- c('DrosX', 'Dros', 'MAR2T','MART2', 'Cont')

# Create the new data frame with the rows to be removed
controls_long <- filtered_dataframe[filtered_dataframe$site %in% values_to_remove, ]

# Update the original data frame to keep only the remaining rows
filtered_dataframe_samples <- filtered_dataframe[!filtered_dataframe$site %in% values_to_remove, ]
```

# check if 'CONTROL SPIKE' is in samples. There should be NO control spike in samples
```{r}
if ("CONTROL SPIKE" %in% filtered_dataframe_samples$species) {
  print("CONTROL SPIKE found")
} else {
  print("No CONTROL SPIKE found")
}
```

# save data for modeling in long format (includes PCR replicates)
```{r}
#Create directory for modeling data
dir.create(paste0(uomshare,"/final_modeling_data/"))

# Generate a timestamp
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp (set Miseq run number)
file_name <- paste0(uomshare, "/final_modeling_data/", miseq_folder, "_final_data_long_format_", timestamp, ".csv")
# Save the dataframe with the new file name
write.csv(filtered_dataframe_samples, file = file_name, row.names = FALSE)
```

# save control for checking in notebook 07
```{r}
# Generate a timestamp
timestamp <- format(Sys.time(), "%d_%m_%Y")

# Create the file name with the timestamp (set Miseq run number)
cont <- paste0(uomshare, "/final_data_summaries/", miseq_folder, "_CONTROLS_long_format_", timestamp, ".csv")
# Save the dataframe with the new file name
write.csv(controls_long, file = cont, row.names = FALSE)
```




